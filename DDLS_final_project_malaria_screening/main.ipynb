{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiation\n",
    "\n",
    "1. Add the project folder to your VS Code workspace.  \n",
    "2. Download the dataset:\n",
    "   - Go to [this page](https://lhncbc.nlm.nih.gov/LHC-research/LHC-projects/image-processing/malaria-datasheet.html)\n",
    "   - Download **\"NLM-Falciparum&Uninfected-Thin-193Patients\"**\n",
    "   - Unzip the downloaded archive into a convenient location (for example, `C:\\Projects\\cell images`)\n",
    "\n",
    "3. Open a terminal in VS Code (PowerShell recommended) and run the following commands in order:\n",
    "\n",
    "> **Note:** Replace the path in the first command with the location where you unzipped `cell images`.\n",
    "\n",
    "```powershell\n",
    "# 1) Go to the project data folder\n",
    "cd \"C:\\[path to where you unzipped 'cell images']\"\n",
    "\n",
    "# 2) Create a Python 3.11 virtual environment\n",
    "py -3.11 -m venv .venv\n",
    "\n",
    "# 3) Activate the environment (PowerShell)\n",
    ".\\.venv\\Scripts\\Activate.ps1\n",
    "\n",
    "# 4) Upgrade pip\n",
    "python -m pip install --upgrade pip\n",
    "\n",
    "# 5) Install PyTorch (CUDA 12.1 build)\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# 6) Install the packages\n",
    "pip install numpy pandas pillow scikit-learn matplotlib tqdm jupyter ipykernel fastapi uvicorn==0.30.* starlette python-multipart\n",
    "\n",
    "# 7) Register the Jupyter kernel\n",
    "python -m ipykernel install --user --name ddls-local --display-name \"Python 3.11 (.venv) DDLS\"\n",
    "```\n",
    "\n",
    "4. In VS Code, select the Jupyter kernel: **Python 3.11 (.venv) DDLS**.\n",
    "\n",
    "5. Change all instances of \"[path_placeholder]\" to the path where you unzipped 'cell images'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment and GPU Verification\n",
    "\n",
    "This cell imports the core libraries (`sys`, `platform`, `torch`, `torchvision`) and prints the current Python executable path, Python version, and library versions for Torch and TorchVision.  \n",
    "It then checks whether CUDA is available on the system.  \n",
    "If a GPU is detected, the code prints the GPU device name and performs a small matrix multiplication on the GPU to confirm correct CUDA functionality.  \n",
    "If no GPU is available, a message is printed advising that the correct virtual environment and GPU drivers should be verified.  \n",
    "This cell ensures the runtime environment is properly configured before any computational steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, platform, torch, torchvision\n",
    "print(\"Python exe:\", sys.executable)\n",
    "print(\"Python ver:\", platform.python_version())\n",
    "print(\"Torch :\", torch.__version__)\n",
    "print(\"TV    :\", torchvision.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0))\n",
    "    # tiny CUDA sanity op\n",
    "    x = torch.randn(1024, 1024, device=\"cuda\")\n",
    "    y = torch.mm(x, x.t())\n",
    "    print(\"CUDA matmul OK, shape:\", y.shape)\n",
    "else:\n",
    "    print(\"No CUDA visible. Make sure this kernel is the .venv and drivers are up to date.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 — Environment Setup and Global Seeding\n",
    "\n",
    "This cell defines the local directory structure for the project and establishes reproducibility through deterministic random seeds.\n",
    "\n",
    "- **Directory setup:**  \n",
    "  The project root, dataset directory, and results directory are specified using `pathlib.Path`.  \n",
    "  The results directory is created if it does not already exist.\n",
    "\n",
    "- **Reproducibility:**  \n",
    "  The function `seed_everything()` sets fixed seeds for the `random`, `numpy`, and `torch` libraries.  \n",
    "  It also configures PyTorch’s backend to deterministic mode, disabling automatic benchmarking to ensure consistent results across runs.  \n",
    "  These settings may slightly reduce computational speed but guarantee that data splits, model initialisations, and training results remain reproducible.\n",
    "\n",
    "- **Output:**  \n",
    "  After setting the seed, the cell prints the global seed value and the absolute paths to the project root, dataset, and results directories.  \n",
    "  This confirms that the working environment is correctly initialised before proceeding with data exploration or model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 — Local environment & global seed\n",
    "import os, random, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Local roots ----\n",
    "PROJECT_ROOT = Path(r\"[path_placeholder]\")\n",
    "DATA_DIR     = PROJECT_ROOT / \"cell_images\"\n",
    "RESULTS_DIR  = PROJECT_ROOT / \"results\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Reproducibility ----\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # For reproducibility (set with care; can reduce speed a bit)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(f\"Global seed set to {seed}\")\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Dataset dir:  {DATA_DIR}\")\n",
    "print(f\"Results dir:  {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_lU-aL7FbEj"
   },
   "source": [
    "# Step 2 — Dataset Structure Check and Class Counts\n",
    "\n",
    "This cell validates the expected on-disk layout and produces basic counts per class.\n",
    "\n",
    "- **Expected layout:**  \n",
    "  Two class subfolders under `DATA_DIR`: `Parasitized` and `Uninfected`.\n",
    "\n",
    "- **Discovery and warnings:**  \n",
    "  Lists present subfolders; prints a warning if any expected class folder is missing.\n",
    "\n",
    "- **File indexing:**  \n",
    "  Scans each class folder for image files with extensions `{.png, .jpg, .jpeg}`; builds an `(image_path, class)` index and tallies per-class counts.\n",
    "\n",
    "- **Output:**  \n",
    "  Prints counts for each class and the total number of images.  \n",
    "  If no images are found, raises a `RuntimeError` to flag a misconfigured `DATA_DIR`.\n",
    "\n",
    "This check verifies that the dataset is complete and correctly arranged before downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15033,
     "status": "ok",
     "timestamp": 1761307140131,
     "user": {
      "displayName": "Simon Britzelli",
      "userId": "06415475213358217915"
     },
     "user_tz": -120
    },
    "id": "hvCn_2j3Fdhf",
    "outputId": "4f3a9393-db91-43b8-d5be-35684da1a3e5"
   },
   "outputs": [],
   "source": [
    "# Step 2 — Sanity-check the dataset layout and counts\n",
    "\n",
    "# Expected classes\n",
    "expected_classes = {\"Parasitized\", \"Uninfected\"}\n",
    "\n",
    "# Discover subfolders\n",
    "present = {p.name for p in DATA_DIR.iterdir() if p.is_dir()}\n",
    "print(\"Subfolders found:\", present)\n",
    "\n",
    "missing = expected_classes - present\n",
    "if missing:\n",
    "    print(\"WARNING: missing class folders:\", missing)\n",
    "\n",
    "# Collect files\n",
    "def list_images(cls_dir: Path):\n",
    "    exts = {\".png\", \".jpg\", \".jpeg\"}\n",
    "    return sorted([p for p in cls_dir.iterdir() if p.suffix.lower() in exts and p.is_file()])\n",
    "\n",
    "index = []\n",
    "counts = {}\n",
    "\n",
    "for cls in sorted(expected_classes):\n",
    "    cls_dir = DATA_DIR / cls\n",
    "    if not cls_dir.exists():\n",
    "        counts[cls] = 0\n",
    "        continue\n",
    "    files = list_images(cls_dir)\n",
    "    counts[cls] = len(files)\n",
    "    index.extend([(str(p), cls) for p in files])\n",
    "\n",
    "# Print counts\n",
    "total = sum(counts.values())\n",
    "print(\"\\nImage counts:\")\n",
    "for k, v in counts.items():\n",
    "    print(f\"  {k:12s}: {v:6d}\")\n",
    "print(f\"  {'TOTAL':12s}: {total:6d}\")\n",
    "\n",
    "# Quick assertion to catch a half-empty dataset by accident (soft)\n",
    "if total == 0:\n",
    "    raise RuntimeError(\"No images found. Check DATA_DIR.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wp9OmOVFkot"
   },
   "source": [
    "# Step 3 — Build a File Index with Basic Image Metadata\n",
    "\n",
    "This cell scans all discovered image paths and creates a tabular index with minimal metadata.\n",
    "\n",
    "- **Process:**  \n",
    "  Iterates over `(path, class)` pairs; opens each image with Pillow; records filename, extension, width, and height.  \n",
    "  Any unreadable file is caught; its path and error message are stored for logging.\n",
    "\n",
    "- **Outputs:**  \n",
    "  - `results/index_raw.csv`: one row per image with columns `path, class, filename, ext, width, height`.  \n",
    "  - `results/bad_files.txt` (only if needed): paths of images that failed to open, with the exception text.\n",
    "\n",
    "- **Why this step:**  \n",
    "  Provides a single source of truth for downstream exploration, splitting, and preprocessing; surfaces corrupt or incompatible files early.\n",
    "\n",
    "- **Notes:**  \n",
    "  A short progress bar (`tqdm`) shows indexing status; a final summary prints the number of indexed images and whether any files were skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5777155,
     "status": "ok",
     "timestamp": 1761313008996,
     "user": {
      "displayName": "Simon Britzelli",
      "userId": "06415475213358217915"
     },
     "user_tz": -120
    },
    "id": "lK8sCT-VF3ib",
    "outputId": "f7ff0481-c62b-4aa4-f1bc-53deafb06713"
   },
   "outputs": [],
   "source": [
    "# Step 3 — Build a full index with metadata\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "rows = []\n",
    "bad = []\n",
    "\n",
    "for path_str, cls in tqdm(index, desc=\"Indexing images\"):\n",
    "    p = Path(path_str)\n",
    "    try:\n",
    "        with Image.open(p) as im:\n",
    "            w, h = im.size\n",
    "            ext = p.suffix.lower()\n",
    "    except Exception as e:\n",
    "        bad.append((path_str, str(e)))\n",
    "        continue\n",
    "\n",
    "    rows.append({\n",
    "        \"path\": path_str,\n",
    "        \"class\": cls,\n",
    "        \"filename\": p.name,\n",
    "        \"ext\": ext,\n",
    "        \"width\": w,\n",
    "        \"height\": h,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "csv_path = RESULTS_DIR / \"index_raw.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Indexed {len(df)} images → {csv_path}\")\n",
    "\n",
    "if bad:\n",
    "    bad_path = RESULTS_DIR / \"bad_files.txt\"\n",
    "    with open(bad_path, \"w\") as fh:\n",
    "        for path_str, msg in bad:\n",
    "            fh.write(f\"{path_str}\\t{msg}\\n\")\n",
    "    print(f\"Unreadable files: {len(bad)} (logged to {bad_path})\")\n",
    "else:\n",
    "    print(\"All images opened successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ssbzzl6bG5MG"
   },
   "source": [
    "# Step 4 — Image Size Summary and Outlier Inspection\n",
    "\n",
    "This cell summarises image dimensions and lists extreme cases.\n",
    "\n",
    "- **Statistics:**  \n",
    "  Computes width and height percentiles (min, p10, median, p90, max) from `index_raw.csv`. This guides padding and resizing choices.\n",
    "\n",
    "- **Outlier tables:**  \n",
    "  Shows the ten smallest and ten largest images by width; repeats for height. IPython `display` renders compact tables with `filename, class, width, height`.\n",
    "\n",
    "- **Why this matters:**  \n",
    "  Size spread affects padding artefacts and interpolation; extreme aspect ratios or very small crops may require special handling or exclusion.\n",
    "\n",
    "Review the extremes before finalising the canonical transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1761313019577,
     "user": {
      "displayName": "Simon Britzelli",
      "userId": "06415475213358217915"
     },
     "user_tz": -120
    },
    "id": "wV-4KQ67G8iH",
    "outputId": "080666ea-656b-4188-d725-67ef0dc146bf"
   },
   "outputs": [],
   "source": [
    "# Step 4 — Image size statistics and extremes\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "def pct(s, q):\n",
    "    return np.percentile(s.to_numpy(), q)\n",
    "\n",
    "w = df[\"width\"]\n",
    "h = df[\"height\"]\n",
    "\n",
    "print(\"Width stats:\")\n",
    "print({\n",
    "    \"min\": int(w.min()), \"p10\": int(pct(w,10)), \"p50\": int(pct(w,50)),\n",
    "    \"p90\": int(pct(w,90)), \"max\": int(w.max())\n",
    "})\n",
    "print(\"Height stats:\")\n",
    "print({\n",
    "    \"min\": int(h.min()), \"p10\": int(pct(h,10)), \"p50\": int(pct(h,50)),\n",
    "    \"p90\": int(pct(h,90)), \"max\": int(h.max())\n",
    "})\n",
    "\n",
    "# Smallest widths\n",
    "print(\"\\nTop 10 smallest widths:\")\n",
    "display(df.sort_values(\"width\").head(10)[[\"filename\",\"class\",\"width\",\"height\"]])\n",
    "\n",
    "# Smallest heights\n",
    "print(\"\\nTop 10 smallest heights:\")\n",
    "display(df.sort_values(\"height\").head(10)[[\"filename\",\"class\",\"width\",\"height\"]])\n",
    "\n",
    "# Largest widths\n",
    "print(\"\\nTop 10 largest widths:\")\n",
    "display(df.sort_values(\"width\", ascending=False).head(10)[[\"filename\",\"class\",\"width\",\"height\"]])\n",
    "\n",
    "# Largest heights\n",
    "print(\"\\nTop 10 largest heights:\")\n",
    "display(df.sort_values(\"height\", ascending=False).head(10)[[\"filename\",\"class\",\"width\",\"height\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76rmaHs9cZ9Z"
   },
   "source": [
    "# Step 5 — Derive Group Keys to Prevent Split Leakage\n",
    "\n",
    "This cell extracts a slide/session **group key** from each filename by removing the trailing pattern `_cell_<digits>.<ext>`. The key is used later to build **group-wise** train/val/test splits so that images from the same slide do not appear in multiple splits.\n",
    "\n",
    "- **Group derivation:**  \n",
    "  Applies a regex to `filename` to strip the per-cell suffix and produce `group`.\n",
    "\n",
    "- **Group statistics:**  \n",
    "  Reports the number of groups and the distribution of group sizes (min, p25, p50, p75, max).  \n",
    "  Lists the largest groups overall, then the top groups per class to spot imbalances or anomalies.\n",
    "\n",
    "- **Output:**  \n",
    "  Saves the augmented index with a `group` column to `results/index_with_groups.csv`.\n",
    "\n",
    "This step guards against information leakage by keeping slide-level batches intact during cross-validation and final splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 507,
     "status": "ok",
     "timestamp": 1761313147617,
     "user": {
      "displayName": "Simon Britzelli",
      "userId": "06415475213358217915"
     },
     "user_tz": -120
    },
    "id": "yB0K3oRCcby4",
    "outputId": "da5984d6-3dbf-452c-b41c-2e9fac4c810f"
   },
   "outputs": [],
   "source": [
    "# Step 5 — Derive a leakage-safe group key per image\n",
    "\n",
    "from IPython.display import display\n",
    "import re\n",
    "\n",
    "def derive_group_key(fname: str) -> str:\n",
    "    # Remove trailing `_cell_<digits>.<ext>` (case-insensitive ext)\n",
    "    return re.sub(r\"_cell_\\d+\\.(png|jpg|jpeg)$\", \"\", fname, flags=re.IGNORECASE)\n",
    "\n",
    "df[\"group\"] = df[\"filename\"].apply(derive_group_key)\n",
    "\n",
    "# Group stats\n",
    "gsize = df.groupby(\"group\").size().sort_values(ascending=False)\n",
    "print(f\"Total groups: {gsize.shape[0]}\")\n",
    "print({\n",
    "    \"min\": int(gsize.min()), \"p25\": int(pct(gsize,25)), \"p50\": int(pct(gsize,50)),\n",
    "    \"p75\": int(pct(gsize,75)), \"max\": int(gsize.max())\n",
    "})\n",
    "\n",
    "print(\"\\nLargest groups (all classes mixed):\")\n",
    "display(gsize.head(10).to_frame(name=\"n_images\"))\n",
    "\n",
    "# Group-by-class top examples\n",
    "print(\"\\nTop groups by class (head=5 per class):\")\n",
    "for cls in sorted(df[\"class\"].unique()):\n",
    "    tmp = df[df[\"class\"] == cls].groupby(\"group\").size().sort_values(ascending=False).head(5)\n",
    "    print(f\"\\nClass: {cls}\")\n",
    "    display(tmp.to_frame(name=\"n_images\"))\n",
    "\n",
    "# Save\n",
    "csv_groups = RESULTS_DIR / \"index_with_groups.csv\"\n",
    "df.to_csv(csv_groups, index=False)\n",
    "print(f\"Saved with groups → {csv_groups}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXon2r8Dcetm"
   },
   "source": [
    "# Step 6 — Probe border background colours on a stratified sample\n",
    "\n",
    "Purpose:\n",
    "- Verify that outer-border pixels are flat and dark; this supports the plan to pad-to-square with a constant black background.\n",
    "- Check this **per class** to rule out class-specific acquisition artefacts.\n",
    "\n",
    "Method:\n",
    "- Randomly sample N images per class (default N=2000).\n",
    "- For each image, extract a 2-pixel ring at the four borders; compute the modal RGB triplet and its fraction of the ring.\n",
    "- Classify a mode as “near black” if all RGB components ≤ 10 (tolerant threshold for JPEG/PNG quantisation).\n",
    "\n",
    "Outputs:\n",
    "- A table with filename, class, modal RGB, and fraction represented.\n",
    "- A per-class summary: count and proportion of images with near-black modes.\n",
    "\n",
    "Interpretation:\n",
    "- If ≥ ~90% show near-black modes per class, constant-black padding is appropriate.\n",
    "- If many images have non-black modes, consider a fallback (e.g., reflect padding) for those specific cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 35383,
     "status": "ok",
     "timestamp": 1761314137343,
     "user": {
      "displayName": "Simon Britzelli",
      "userId": "06415475213358217915"
     },
     "user_tz": -120
    },
    "id": "wrZX2vWHcieE",
    "outputId": "29b0a32f-9153-4086-f363-200858e96a8d"
   },
   "outputs": [],
   "source": [
    "# Step 6 — Probe border background colours on a stratified sample\n",
    "\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "# Parameters\n",
    "N_PER_CLASS = 2000\n",
    "RING_PX = 2\n",
    "NEAR_BLACK_THR = 10  # per-channel threshold for \"near black\"\n",
    "\n",
    "rng = 123\n",
    "\n",
    "# Stratified sample (no deprecation warning)\n",
    "sample_rows = (\n",
    "    df.groupby(\"class\", group_keys=False)\n",
    "      .apply(lambda g: g.sample(min(N_PER_CLASS, len(g)), random_state=rng))\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "def border_ring_mode_rgb(path: Path, ring: int = RING_PX, max_pixels: int = 20000):\n",
    "    from collections import Counter\n",
    "    with Image.open(path) as im:\n",
    "        im = im.convert(\"RGB\")\n",
    "        arr = np.array(im)\n",
    "    h, w, _ = arr.shape\n",
    "    top = arr[0:ring, :, :]\n",
    "    bottom = arr[h-ring:h, :, :]\n",
    "    left = arr[:, 0:ring, :]\n",
    "    right = arr[:, w-ring:w, :]\n",
    "    ring_pixels = np.concatenate([\n",
    "        top.reshape(-1,3), bottom.reshape(-1,3),\n",
    "        left.reshape(-1,3), right.reshape(-1,3)\n",
    "    ], axis=0)\n",
    "    if ring_pixels.shape[0] > max_pixels:\n",
    "        idx = np.random.choice(ring_pixels.shape[0], size=max_pixels, replace=False)\n",
    "        ring_pixels = ring_pixels[idx]\n",
    "    tuples = [tuple(int(x) for x in v) for v in ring_pixels]\n",
    "    cnt = Counter(tuples)\n",
    "    mode_rgb, mode_n = cnt.most_common(1)[0]\n",
    "    frac = mode_n / len(tuples)\n",
    "    return mode_rgb, frac\n",
    "\n",
    "# Run probe\n",
    "records = []\n",
    "for _, r in sample_rows.iterrows():\n",
    "    rgb, frac = border_ring_mode_rgb(Path(r[\"path\"]), ring=RING_PX)\n",
    "    near_black = all(c <= NEAR_BLACK_THR for c in rgb)\n",
    "    records.append({\n",
    "        \"filename\": r[\"filename\"],\n",
    "        \"class\": r[\"class\"],\n",
    "        \"mode_rgb\": rgb,\n",
    "        \"fraction\": round(frac, 3),\n",
    "        \"near_black\": near_black\n",
    "    })\n",
    "\n",
    "probe_df = pd.DataFrame(records)\n",
    "\n",
    "# Display head and per-class summary\n",
    "display(probe_df.head(10))\n",
    "\n",
    "summary = (probe_df.groupby(\"class\")\n",
    "           .agg(n=(\"filename\",\"count\"),\n",
    "                n_near_black=(\"near_black\",\"sum\"),\n",
    "                prop_near_black=(\"near_black\",\"mean\")))\n",
    "display(summary)\n",
    "\n",
    "# Optional: histogram of modal fraction per class\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for cls, sub in probe_df.groupby(\"class\"):\n",
    "    plt.figure()\n",
    "    plt.hist(sub[\"fraction\"], bins=20)\n",
    "    plt.title(f\"Border mode fraction — {cls}\")\n",
    "    plt.xlabel(\"Fraction of ring pixels at modal RGB\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "# Optional: list any non–near-black modes\n",
    "non_nb = probe_df[~probe_df[\"near_black\"]]\n",
    "if not non_nb.empty:\n",
    "    print(\"Non–near-black border modes found:\")\n",
    "    display(non_nb.sort_values(\"fraction\", ascending=False).head(20))\n",
    "else:\n",
    "    print(\"All sampled images have near-black border modes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canonical preprocessing (single source of truth)\n",
    "\n",
    "This cell defines `pad_resize_canonical`, which **all** later steps must use:\n",
    "\n",
    "- Convert to RGB; inspect a 2-pixel border ring to get the modal RGB and its fraction.  \n",
    "- If the modal colour is near-black (each channel ≤ 10) **and** covers ≥ 60% of the ring, pad with constant black; otherwise, use reflect padding.  \n",
    "- Centre-pad each image to a square; bicubic resize to 128×128.  \n",
    "\n",
    "This matches the approved plan and keeps preprocessing identical for training, statistics, evaluation, TTA, OOD, Grad-CAM, and the app. Import it in later cells rather than redefining new variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canonical preprocessing — single source of truth\n",
    "# Pad to square using a near-black heuristic; then bicubic resize to 128×128.\n",
    "# Use this everywhere: training, stats, evaluation, TTA, OOD, Grad-CAM, and the app.\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Constants (match the approved plan)\n",
    "NEAR_BLACK_THR = 10      # each RGB channel ≤ 10 → \"near black\"\n",
    "MIN_MODE_FRAC  = 0.60    # modal ring colour must cover ≥60% of ring pixels\n",
    "\n",
    "# Pillow compatibility (older/newer)\n",
    "try:\n",
    "    RESAMPLE_BICUBIC = Image.Resampling.BICUBIC  # Pillow ≥ 9.1\n",
    "except Exception:\n",
    "    RESAMPLE_BICUBIC = Image.BICUBIC             # older Pillow\n",
    "\n",
    "def _ring_mode_from_array(arr: np.ndarray, ring: int = 2, max_pixels: int = 20000):\n",
    "    \"\"\"\n",
    "    Modal RGB on the outer 'ring' (top/bottom/left/right strips).\n",
    "    Uses deterministic sub-sampling by stride if too many pixels.\n",
    "    \"\"\"\n",
    "    h, w, _ = arr.shape\n",
    "    top    = arr[0:ring, :, :]\n",
    "    bottom = arr[h-ring:h, :, :]\n",
    "    left   = arr[:, 0:ring, :]\n",
    "    right  = arr[:, w-ring:w, :]\n",
    "    ring_pixels = np.concatenate(\n",
    "        [top.reshape(-1,3), bottom.reshape(-1,3), left.reshape(-1,3), right.reshape(-1,3)],\n",
    "        axis=0\n",
    "    )\n",
    "    if ring_pixels.shape[0] > max_pixels:\n",
    "        step = int(np.ceil(ring_pixels.shape[0] / max_pixels))\n",
    "        ring_pixels = ring_pixels[::step]\n",
    "    tuples = [tuple(int(x) for x in v) for v in ring_pixels]\n",
    "    cnt = Counter(tuples)\n",
    "    mode_rgb, n = cnt.most_common(1)[0]\n",
    "    frac = n / max(1, len(tuples))\n",
    "    return mode_rgb, frac\n",
    "\n",
    "def pad_resize_canonical(img: Image.Image, target_size: int = 128, ring: int = 2) -> Image.Image:\n",
    "    \"\"\"\n",
    "    1) Convert to RGB.\n",
    "    2) Inspect border ring to pick pad mode:\n",
    "         - If modal ring colour is near-black (all channels ≤ NEAR_BLACK_THR)\n",
    "           and covers ≥ MIN_MODE_FRAC of ring pixels → constant black pad.\n",
    "         - Otherwise → reflect pad.\n",
    "    3) Centre-pad to square; bicubic resize to (target_size, target_size).\n",
    "    \"\"\"\n",
    "    img = img.convert(\"RGB\")\n",
    "    arr = np.array(img)\n",
    "    mode_rgb, frac = _ring_mode_from_array(arr, ring=ring)\n",
    "    near_black = (mode_rgb[0] <= NEAR_BLACK_THR and\n",
    "                  mode_rgb[1] <= NEAR_BLACK_THR and\n",
    "                  mode_rgb[2] <= NEAR_BLACK_THR)\n",
    "    use_constant = (near_black and frac >= MIN_MODE_FRAC)\n",
    "\n",
    "    w, h = img.size\n",
    "    if w != h:\n",
    "        side = max(w, h)\n",
    "        pad_left   = (side - w) // 2\n",
    "        pad_right  = side - w - pad_left\n",
    "        pad_top    = (side - h) // 2\n",
    "        pad_bottom = side - h - pad_top\n",
    "        if use_constant:\n",
    "            padded = ImageOps.expand(img, border=(pad_left, pad_top, pad_right, pad_bottom), fill=mode_rgb)\n",
    "        else:\n",
    "            # reflect via numpy for exact control\n",
    "            pad_width = ((pad_top, pad_bottom), (pad_left, pad_right), (0, 0))\n",
    "            padded = Image.fromarray(np.pad(arr, pad_width, mode=\"reflect\"))\n",
    "    else:\n",
    "        padded = img\n",
    "\n",
    "    return padded.resize((target_size, target_size), resample=RESAMPLE_BICUBIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBNr2YfLhVLU"
   },
   "source": [
    "# Step 7 — Canonical Transform (pad-to-square + 128×128 resize) and Sample Validation\n",
    "\n",
    "This cell implements and validates the preprocessing used in training and inference.\n",
    "\n",
    "- **Inputs and guards:**  \n",
    "  Reloads `index_with_groups.csv` if `df` is not present; prepares output paths.\n",
    "\n",
    "- **Parameters:**  \n",
    "  `TARGET_SIZE=128`; border ring width `RING_PX=2`; near-black threshold `NEAR_BLACK_THR=10` (per channel); minimum modal-colour coverage `MIN_MODE_FRAC=0.60`. Bicubic resampling is selected with a Pillow-version-safe fallback.\n",
    "\n",
    "- **Border analysis:**  \n",
    "  `ring_mode_rgb()` computes the modal RGB of the outer ring and its fraction.  \n",
    "  `decide_pad_mode()` returns `\"constant_black\"` if the modal colour is near-black **and** covers ≥60% of ring pixels; otherwise it returns `\"reflect\"`.\n",
    "\n",
    "- **Transform:**  \n",
    "  `pad_to_square()` pads symmetrically to a square using either a constant dark fill or reflect; then resizes to 128×128 with bicubic.  \n",
    "  `preprocess_one()` applies the full pipeline and records metadata (original size, pad mode, per-side pad pixels, modal colour and fraction).\n",
    "\n",
    "- **Validation on a sample:**  \n",
    "  Draws `SAMPLES_PER_CLASS=12` per class; runs the transform; stores a per-image report to `results/preprocess_sample_report.csv`.  \n",
    "  Prints pad-mode counts and the most frequent border colours; saves a side-by-side montage (original | processed) to `results/preprocess_sample_montage.png`.\n",
    "\n",
    "- **Purpose:**  \n",
    "  Locks the exact preprocessing that will be used everywhere; provides a quick visual and numeric check that padding choice and resizing behave as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105686,
     "status": "ok",
     "timestamp": 1761378921205,
     "user": {
      "displayName": "Simon Britzelli",
      "userId": "06415475213358217915"
     },
     "user_tz": -120
    },
    "id": "tNkwLfruhXOJ",
    "outputId": "a6ee29c3-7686-4910-f0c7-75968f924a60"
   },
   "outputs": [],
   "source": [
    "# Step 7 — Canonical transform implementation + sample validation\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- Robustly re-establish paths / inputs ----\n",
    "DATASET_DIR  = DATA_DIR\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INDEX_WITH_GROUPS = RESULTS_DIR / \"index_with_groups.csv\"\n",
    "if \"df\" not in globals():\n",
    "    assert INDEX_WITH_GROUPS.exists(), f\"Missing {INDEX_WITH_GROUPS}; please rerun earlier indexing steps.\"\n",
    "    df = pd.read_csv(INDEX_WITH_GROUPS)\n",
    "\n",
    "# ---- Parameters for transform ----\n",
    "TARGET_SIZE = 128\n",
    "RING_PX = 2\n",
    "NEAR_BLACK_THR = 10     # per-channel max to consider \"near-black\"\n",
    "MIN_MODE_FRAC = 0.60    # if the modal ring colour covers <60% of ring pixels, we treat as uncertain and fallback to reflect\n",
    "\n",
    "# ---- Utilities ----\n",
    "try:\n",
    "    # Pillow ≥ 9/10\n",
    "    from PIL import Image\n",
    "    RESAMPLE_BICUBIC = Image.BICUBIC if hasattr(Image, \"BICUBIC\") else Image.Resampling.BICUBIC\n",
    "except Exception:\n",
    "    RESAMPLE_BICUBIC = Image.BICUBIC\n",
    "\n",
    "def preprocess_one(path: Path, target_size=128):\n",
    "    with Image.open(path) as im:\n",
    "        im = im.convert(\"RGB\")\n",
    "        arr = np.array(im)\n",
    "    mode_rgb, frac = _ring_mode_from_array(arr, ring=2)\n",
    "    near_black = all(c <= NEAR_BLACK_THR for c in mode_rgb)\n",
    "    pad_mode = \"constant_black\" if (near_black and frac >= MIN_MODE_FRAC) else \"reflect\"\n",
    "\n",
    "    im_out = pad_resize_canonical(Image.fromarray(arr), target_size=target_size, ring=2)\n",
    "    meta = {\n",
    "        \"orig_w\": arr.shape[1], \"orig_h\": arr.shape[0],\n",
    "        \"pad_mode\": pad_mode, \"mode_rgb\": mode_rgb, \"mode_frac\": round(frac, 3),\n",
    "        \"final_w\": target_size, \"final_h\": target_size\n",
    "    }\n",
    "    return im_out, meta\n",
    "\n",
    "# ---- Sample a small set per class and run the transform ----\n",
    "SAMPLES_PER_CLASS = 12\n",
    "rng = 42\n",
    "sample_df = (\n",
    "    df.groupby(\"class\", group_keys=False)\n",
    "      .apply(lambda g: g.sample(min(SAMPLES_PER_CLASS, len(g)), random_state=rng))\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "records = []\n",
    "thumbs = []  # (orig, processed) pairs for montage\n",
    "for _, r in sample_df.iterrows():\n",
    "    p = Path(r[\"path\"])\n",
    "    with Image.open(p) as im_orig:\n",
    "        im_orig = im_orig.convert(\"RGB\")\n",
    "    im_proc, meta = preprocess_one(p)\n",
    "    records.append({\n",
    "        \"filename\": r[\"filename\"],\n",
    "        \"class\": r[\"class\"],\n",
    "        **meta\n",
    "    })\n",
    "    # Build side-by-side thumbnails for montage\n",
    "    # Normalize thumbnail height to 128 for consistent grid\n",
    "    thumbs.append((im_orig, im_proc))\n",
    "\n",
    "# ---- Save report ----\n",
    "report_df = pd.DataFrame(records)\n",
    "report_csv = RESULTS_DIR / \"preprocess_sample_report.csv\"\n",
    "report_df.to_csv(report_csv, index=False)\n",
    "print(f\"Saved sample report → {report_csv}\")\n",
    "\n",
    "# ---- Print quick summary ----\n",
    "print(\"\\nPad mode counts:\")\n",
    "print(report_df[\"pad_mode\"].value_counts())\n",
    "\n",
    "print(\"\\nBorder mode colour (top 5):\")\n",
    "print(report_df[\"mode_rgb\"].value_counts().head())\n",
    "\n",
    "# ---- Make a montage: original | processed, 6 rows x 4 cols (pairs) ----\n",
    "def make_montage(pairs, cols=4, out_path=RESULTS_DIR/\"preprocess_sample_montage.png\"):\n",
    "    # Each pair contributes two panels; we stack as [orig, processed] horizontally within a pair.\n",
    "    n = len(pairs)\n",
    "    rows = int(np.ceil(n / cols))\n",
    "    panel_w = 128 * 2   # orig + processed\n",
    "    panel_h = 128\n",
    "    canvas = Image.new(\"RGB\", (panel_w * cols, panel_h * rows), (255,255,255))\n",
    "    for i, (im_orig, im_proc) in enumerate(pairs):\n",
    "        # Fit original to 128x128 preview (keep aspect ratio, pad white if needed)\n",
    "        w, h = im_orig.size\n",
    "        scale = min(128/w, 128/h)\n",
    "        new_w, new_h = int(w*scale), int(h*scale)\n",
    "        im_orig_small = im_orig.resize((new_w, new_h), resample=RESAMPLE_BICUBIC)\n",
    "        panel = Image.new(\"RGB\", (panel_w, panel_h), (255,255,255))\n",
    "        # Centre original in left 128×128\n",
    "        left_pad_x = (128 - new_w)//2\n",
    "        left_pad_y = (128 - new_h)//2\n",
    "        panel.paste(im_orig_small, (left_pad_x, left_pad_y))\n",
    "        # Paste processed on the right (exact 128×128)\n",
    "        panel.paste(im_proc, (128, 0))\n",
    "        # Place panel in grid\n",
    "        r = i // cols\n",
    "        c = i % cols\n",
    "        canvas.paste(panel, (c*panel_w, r*panel_h))\n",
    "    canvas.save(out_path)\n",
    "    return out_path\n",
    "\n",
    "montage_path = make_montage(thumbs, cols=4)\n",
    "print(f\"Saved montage → {montage_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QF4YyEt_o0jj"
   },
   "source": [
    "# Step 8 — Group-wise, Stratified Train/Val/Test Splits\n",
    "\n",
    "This cell creates leakage-safe splits while keeping class balance.\n",
    "\n",
    "- **Inputs.** Loads `index_with_groups.csv`; checks required columns. Encodes labels: `Parasitized→1`, `Uninfected→0`. Uses `group` as the slide/session key.\n",
    "- **Method.** Builds five folds with `StratifiedGroupKFold` so groups do not mix across folds and class proportions stay stable at the group level. Assigns each sample a fold id; picks one fold for validation and one for test using a fixed RNG seed; the remaining folds form the training set.\n",
    "- **Leakage check.** Verifies that every group maps to exactly one split; raises an error if any group crosses splits.\n",
    "- **Summaries.** Prints counts per split; a class-by-split crosstab; and overall split proportions.\n",
    "- **Output.** Saves the manifest with split labels to `results/splits.csv`.\n",
    "\n",
    "This split strategy keeps slide-level correlation confined within a single split; reported counts confirm balance before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2086,
     "status": "ok",
     "timestamp": 1761378923305,
     "user": {
      "displayName": "Simon Britzelli",
      "userId": "06415475213358217915"
     },
     "user_tz": -120
    },
    "id": "krFO-q0bo3NV",
    "outputId": "45ebcdd6-08c4-4555-ceca-b3d23a1f5ac6"
   },
   "outputs": [],
   "source": [
    "# Step 8 — Build leakage-safe Train/Val/Test splits\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Sklearn splitters\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# Paths\n",
    "RESULTS = RESULTS_DIR\n",
    "\n",
    "# Load the grouped index built earlier\n",
    "df = pd.read_csv(RESULTS / \"index_with_groups.csv\")\n",
    "\n",
    "# Basic checks\n",
    "assert {\"path\",\"filename\",\"class\",\"group\",\"width\",\"height\"}.issubset(df.columns), \"Missing required columns.\"\n",
    "\n",
    "# Encode labels\n",
    "label_map = {\"Parasitized\": 1, \"Uninfected\": 0}\n",
    "y = df[\"class\"].map(label_map).values\n",
    "groups = df[\"group\"].values\n",
    "\n",
    "# Build 5 stratified group folds\n",
    "# (Stratifies on 'class' at group level; keeps groups intact.)\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Assign each sample a fold id via the test indices of each split\n",
    "fold_id = np.full(len(df), -1, dtype=int)\n",
    "for k, (_, test_idx) in enumerate(sgkf.split(X=np.zeros(len(df)), y=y, groups=groups)):\n",
    "    fold_id[test_idx] = k\n",
    "\n",
    "assert (fold_id >= 0).all(), \"Unassigned samples detected.\"\n",
    "\n",
    "# Choose folds: one for val, one for test (reproducible)\n",
    "rng = np.random.RandomState(1337)\n",
    "folds = np.arange(5)\n",
    "rng.shuffle(folds)\n",
    "val_fold, test_fold = folds[:2]\n",
    "train_folds = set(folds[2:])\n",
    "\n",
    "def fold_to_split(fid):\n",
    "    if fid == val_fold:\n",
    "        return \"val\"\n",
    "    if fid == test_fold:\n",
    "        return \"test\"\n",
    "    return \"train\"\n",
    "\n",
    "df[\"split\"] = [fold_to_split(fid) for fid in fold_id]\n",
    "\n",
    "# Sanity: no group should cross splits\n",
    "group_to_splits = df.groupby(\"group\")[\"split\"].nunique()\n",
    "if (group_to_splits > 1).any():\n",
    "    bad = group_to_splits[group_to_splits > 1]\n",
    "    raise RuntimeError(f\"Group leakage detected for {len(bad)} groups.\")\n",
    "\n",
    "# Summaries\n",
    "print(\"Fold assignment:\", {\"val\": int(val_fold), \"test\": int(test_fold), \"train\": sorted(list(train_folds))})\n",
    "\n",
    "print(\"\\nCounts per split:\")\n",
    "print(df[\"split\"].value_counts())\n",
    "\n",
    "print(\"\\nClass counts per split:\")\n",
    "print(pd.crosstab(df[\"split\"], df[\"class\"]))\n",
    "\n",
    "print(\"\\nProportions per split (overall):\")\n",
    "print((df[\"split\"].value_counts(normalize=True) * 100).round(2))\n",
    "\n",
    "# Persist manifest\n",
    "out_path = RESULTS / \"splits.csv\"\n",
    "df.to_csv(out_path, index=False)\n",
    "print(f\"\\nSaved manifest → {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCoJDAN3pAak"
   },
   "source": [
    "# Step 9 — Preprocessing Definition and Split-wise Visual Spot-check\n",
    "\n",
    "This cell defines the operational preprocessing and verifies its behaviour on each split.\n",
    "\n",
    "- **Preprocessing.**  \n",
    "  `border_ring_mode_rgb()` estimates the modal border colour from a thin outer ring; `preprocess_image()` pads to a square using that colour, then resizes to 128×128 with bicubic interpolation.\n",
    "\n",
    "- **Visual QA.**  \n",
    "  For each of `train`, `val`, and `test`, it samples up to six images per class, applies the transform, and assembles a compact montage via `montage_grid()`.\n",
    "\n",
    "- **Outputs.**  \n",
    "  Saves `montage_split_train.png`, `montage_split_val.png`, and `montage_split_test.png` under `results/`.\n",
    "\n",
    "**Why this matters.** Confirms that padding and resizing are consistent across splits; provides an immediate check for unintended artefacts or class-specific differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10312,
     "status": "ok",
     "timestamp": 1761378933627,
     "user": {
      "displayName": "Simon Britzelli",
      "userId": "06415475213358217915"
     },
     "user_tz": -120
    },
    "id": "hZ7UvhaGpFYo",
    "outputId": "2f3aab9d-60ed-43ef-979f-9cc526c1c73e"
   },
   "outputs": [],
   "source": [
    "# Step 9 — Define preprocessing (pad-to-square + bicubic 128×128) and spot-check per split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps\n",
    "from collections import Counter\n",
    "\n",
    "RESULTS = RESULTS_DIR\n",
    "df = pd.read_csv(RESULTS / \"splits.csv\")\n",
    "\n",
    "# --- Canonical pad→resize: constant near-black if dominant; otherwise reflect ---\n",
    "NEAR_BLACK_THR = 10\n",
    "MIN_MODE_FRAC  = 0.60\n",
    "\n",
    "def preprocess_image(path: Path, size: int = 128, ring: int = 2) -> Image.Image:\n",
    "    with Image.open(path) as im:\n",
    "        return pad_resize_canonical(im, target_size=size, ring=ring)\n",
    "\n",
    "def montage_grid(imgs, ncols=3, cell_size=128, pad=4):\n",
    "    \"\"\"Create a simple montage for visual spot-checks.\"\"\"\n",
    "    n = len(imgs)\n",
    "    nrows = int(np.ceil(n / ncols))\n",
    "    W = ncols * cell_size + (ncols + 1) * pad\n",
    "    H = nrows * cell_size + (nrows + 1) * pad\n",
    "    canvas = Image.new(\"RGB\", (W, H), (30, 30, 30))\n",
    "    x = y = pad\n",
    "    for i, im in enumerate(imgs):\n",
    "        canvas.paste(im, (x, y))\n",
    "        x += cell_size + pad\n",
    "        if (i + 1) % ncols == 0:\n",
    "            x = pad\n",
    "            y += cell_size + pad\n",
    "    return canvas\n",
    "\n",
    "# --- Build small spot-check montages per split/class ---\n",
    "rng = np.random.RandomState(123)\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    imgs_to_show = []\n",
    "    for cls in [\"Parasitized\", \"Uninfected\"]:\n",
    "        sub = df[(df[\"split\"] == split) & (df[\"class\"] == cls)]\n",
    "        if len(sub) == 0:\n",
    "            continue\n",
    "        sample = sub.sample(min(6, len(sub)), random_state=rng)\n",
    "        for _, r in sample.iterrows():\n",
    "            im = preprocess_image(Path(r[\"path\"]), size=128, ring=2)\n",
    "            imgs_to_show.append(im)\n",
    "    if imgs_to_show:\n",
    "        m = montage_grid(imgs_to_show, ncols=3, cell_size=128, pad=6)\n",
    "        out_file = RESULTS / f\"montage_split_{split}.png\"\n",
    "        m.save(out_file)\n",
    "        print(f\"Saved montage → {out_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1l95rIvLrFc7"
   },
   "source": [
    "# Step 10 — Photometric Statistics on the Training Split\n",
    "\n",
    "This cell quantifies basic intensity properties of the **training** images after the canonical preprocessing (pad-to-square using border-mode colour; bicubic resize to 128×128).\n",
    "\n",
    "- **Scope.** Loads training paths from `splits.csv`; optionally stratified-samples up to `N_MAX=8000`. Each image is padded using the modal border RGB and resized to 128×128 before measurement.\n",
    "\n",
    "- **Per-image features (0–255 scale).**  \n",
    "  - Channel means and standard deviations: `rgb_mean_{r,g,b}`, `rgb_std_{r,g,b}`.  \n",
    "  - Grayscale proxies: `gray_mean` and `gray_std` computed as the per-pixel average over RGB.  \n",
    "  - Global extrema: `vmin`, `vmax`.\n",
    "\n",
    "- **Outputs.**  \n",
    "  - `results/train_photometric_stats.csv`: one row per image with all features.  \n",
    "  - `results/train_photometric_summary.csv`: class-wise aggregates (means and p10/p90 for gray metrics; means for RGB means).  \n",
    "  - Histograms saved to:\n",
    "    - `hist_gray_mean_by_class.png` — grayscale mean by class,  \n",
    "    - `hist_gray_std_by_class.png` — grayscale std by class,  \n",
    "    - `hist_rgb_channel_means.png` — R/G/B means across the whole training set.\n",
    "\n",
    "- **Use in the project.**  \n",
    "  Guides normalisation choices; validates augmentation ranges for brightness/contrast; checks for class-specific photometric shifts.\n",
    "\n",
    "- **Notes.**  \n",
    "  Statistics are computed **after** padding and resizing; near-black borders may slightly affect means in very small crops. Random seed fixes the sampling for repeatability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1997802,
     "status": "ok",
     "timestamp": 1761380931451,
     "user": {
      "displayName": "Simon Britzelli",
      "userId": "06415475213358217915"
     },
     "user_tz": -120
    },
    "id": "4twV1fv2rHby",
    "outputId": "d0b04abd-799f-4d01-df38-3778c8dc3d50"
   },
   "outputs": [],
   "source": [
    "# Step 10 — Photometric statistics on the training split (after canonical preprocessing)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "RESULTS = RESULTS_DIR\n",
    "splits_df = pd.read_csv(RESULTS / \"splits.csv\")\n",
    "\n",
    "# --- Canonical pad→resize (reuse in stats to match training data exactly) ---\n",
    "NEAR_BLACK_THR = 10\n",
    "MIN_MODE_FRAC  = 0.60\n",
    "\n",
    "def preprocess_image(path: Path, size: int = 128, ring: int = 2) -> Image.Image:\n",
    "    with Image.open(path) as im:\n",
    "        return pad_resize_canonical(im, target_size=size, ring=ring)\n",
    "\n",
    "# --- Configuration ---\n",
    "N_MAX = 8000   # set to None to process all train images\n",
    "RNG = np.random.RandomState(7)\n",
    "\n",
    "train_df = splits_df[splits_df[\"split\"] == \"train\"].copy()\n",
    "\n",
    "# Stratified sampling to N_MAX\n",
    "if N_MAX is not None and len(train_df) > N_MAX:\n",
    "    per_class = int(N_MAX // 2)\n",
    "    parts = []\n",
    "    for cls in [\"Parasitized\", \"Uninfected\"]:\n",
    "        sub = train_df[train_df[\"class\"] == cls]\n",
    "        parts.append(sub.sample(min(per_class, len(sub)), random_state=RNG))\n",
    "    train_df = pd.concat(parts, axis=0).sample(frac=1.0, random_state=RNG).reset_index(drop=True)\n",
    "\n",
    "# Per-image statistics after canonical preprocessing\n",
    "records = []\n",
    "for _, r in train_df.iterrows():\n",
    "    im = preprocess_image(Path(r[\"path\"]), size=128, ring=2)\n",
    "    arr = np.asarray(im).astype(np.float32)  # [H,W,3], 0..255\n",
    "    ch = arr.reshape(-1, 3)\n",
    "    gray = arr.mean(axis=2)\n",
    "    records.append({\n",
    "        \"filename\": r[\"filename\"],\n",
    "        \"class\": r[\"class\"],\n",
    "        \"rgb_mean_r\": float(ch[:,0].mean()),\n",
    "        \"rgb_mean_g\": float(ch[:,1].mean()),\n",
    "        \"rgb_mean_b\": float(ch[:,2].mean()),\n",
    "        \"rgb_std_r\":  float(ch[:,0].std()),\n",
    "        \"rgb_std_g\":  float(ch[:,1].std()),\n",
    "        \"rgb_std_b\":  float(ch[:,2].std()),\n",
    "        \"gray_mean\":  float(gray.mean()),\n",
    "        \"gray_std\":   float(gray.std()),\n",
    "        \"vmin\":       float(arr.min()),\n",
    "        \"vmax\":       float(arr.max())\n",
    "    })\n",
    "\n",
    "stats_df = pd.DataFrame(records)\n",
    "stats_csv = RESULTS / \"train_photometric_stats.csv\"\n",
    "stats_df.to_csv(stats_csv, index=False)\n",
    "print(f\"Saved per-image stats → {stats_csv}\")\n",
    "\n",
    "# Summary per class\n",
    "def pct(s, q): return float(np.percentile(s, q))\n",
    "\n",
    "summary = (\n",
    "    stats_df\n",
    "    .groupby(\"class\")\n",
    "    .agg(gray_mean_mean=(\"gray_mean\",\"mean\"),\n",
    "         gray_mean_p10 =(\"gray_mean\", lambda s: pct(s,10)),\n",
    "         gray_mean_p90 =(\"gray_mean\", lambda s: pct(s,90)),\n",
    "         gray_std_mean =(\"gray_std\",\"mean\"),\n",
    "         gray_std_p10  =(\"gray_std\",  lambda s: pct(s,10)),\n",
    "         gray_std_p90  =(\"gray_std\",  lambda s: pct(s,90)),\n",
    "         r_mean=(\"rgb_mean_r\",\"mean\"),\n",
    "         g_mean=(\"rgb_mean_g\",\"mean\"),\n",
    "         b_mean=(\"rgb_mean_b\",\"mean\"))\n",
    "    .round(2)\n",
    ")\n",
    "summary_csv = RESULTS / \"train_photometric_summary.csv\"\n",
    "summary.to_csv(summary_csv)\n",
    "print(f\"Saved summary → {summary_csv}\\n\")\n",
    "print(summary)\n",
    "\n",
    "# Histograms — grayscale mean and std by class\n",
    "plt.figure(figsize=(6,4))\n",
    "for cls in [\"Parasitized\", \"Uninfected\"]:\n",
    "    plt.hist(stats_df.loc[stats_df[\"class\"]==cls, \"gray_mean\"], bins=40, alpha=0.6, label=cls)\n",
    "plt.xlabel(\"Grayscale mean (0–255)\"); plt.ylabel(\"Count\"); plt.title(\"Train — grayscale mean by class\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "out = RESULTS / \"hist_gray_mean_by_class.png\"\n",
    "plt.savefig(out, dpi=150); plt.close()\n",
    "print(f\"Saved {out}\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "for cls in [\"Parasitized\", \"Uninfected\"]:\n",
    "    plt.hist(stats_df.loc[stats_df[\"class\"]==cls, \"gray_std\"], bins=40, alpha=0.6, label=cls)\n",
    "plt.xlabel(\"Grayscale std (contrast proxy)\"); plt.ylabel(\"Count\"); plt.title(\"Train — grayscale std by class\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "out = RESULTS / \"hist_gray_std_by_class.png\"\n",
    "plt.savefig(out, dpi=150); plt.close()\n",
    "print(f\"Saved {out}\")\n",
    "\n",
    "# Histograms — per-channel means (both classes together)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(stats_df[\"rgb_mean_r\"], bins=40, alpha=0.6, label=\"R\")\n",
    "plt.hist(stats_df[\"rgb_mean_g\"], bins=40, alpha=0.6, label=\"G\")\n",
    "plt.hist(stats_df[\"rgb_mean_b\"], bins=40, alpha=0.6, label=\"B\")\n",
    "plt.xlabel(\"Channel mean (0–255)\"); plt.ylabel(\"Count\"); plt.title(\"Train — channel means (R,G,B)\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "out = RESULTS / \"hist_rgb_channel_means.png\"\n",
    "plt.savefig(out, dpi=150); plt.close()\n",
    "print(f\"Saved {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10 extension — split photometrics\n",
    "\n",
    "Run the same photometric analysis on **val** and **test** after the canonical preprocessing; save `{split}_photometric_stats.csv`, `{split}_photometric_summary.csv`, and three histograms per split (grey mean, grey std, RGB means)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10 — extension: Photometric statistics on the val and test splits\n",
    "\n",
    "for SPLIT, N_MAX_SPLIT in [(\"val\", 8000), (\"test\", None)]:\n",
    "    sub_df = splits_df[splits_df[\"split\"] == SPLIT].copy()\n",
    "\n",
    "    # Stratified sampling to N_MAX_SPLIT\n",
    "    if N_MAX_SPLIT is not None and len(sub_df) > N_MAX_SPLIT:\n",
    "        per_class = int(N_MAX_SPLIT // 2)\n",
    "        parts = []\n",
    "        for cls in [\"Parasitized\", \"Uninfected\"]:\n",
    "            g = sub_df[sub_df[\"class\"] == cls]\n",
    "            parts.append(g.sample(min(per_class, len(g)), random_state=RNG))\n",
    "        sub_df = pd.concat(parts, axis=0).sample(frac=1.0, random_state=RNG).reset_index(drop=True)\n",
    "\n",
    "    # Per-image statistics after canonical preprocessing\n",
    "    records = []\n",
    "    for _, r in sub_df.iterrows():\n",
    "        im = preprocess_image(Path(r[\"path\"]), size=128, ring=2)\n",
    "        arr = np.asarray(im).astype(np.float32)  # [H,W,3], 0..255\n",
    "        ch = arr.reshape(-1, 3)\n",
    "        gray = arr.mean(axis=2)\n",
    "        records.append({\n",
    "            \"filename\": r[\"filename\"],\n",
    "            \"class\": r[\"class\"],\n",
    "            \"rgb_mean_r\": float(ch[:,0].mean()),\n",
    "            \"rgb_mean_g\": float(ch[:,1].mean()),\n",
    "            \"rgb_mean_b\": float(ch[:,2].mean()),\n",
    "            \"rgb_std_r\":  float(ch[:,0].std()),\n",
    "            \"rgb_std_g\":  float(ch[:,1].std()),\n",
    "            \"rgb_std_b\":  float(ch[:,2].std()),\n",
    "            \"gray_mean\":  float(gray.mean()),\n",
    "            \"gray_std\":   float(gray.std()),\n",
    "            \"vmin\":       float(arr.min()),\n",
    "            \"vmax\":       float(arr.max())\n",
    "        })\n",
    "\n",
    "    stats_df = pd.DataFrame(records)\n",
    "    stats_csv = RESULTS / f\"{SPLIT}_photometric_stats.csv\"\n",
    "    stats_df.to_csv(stats_csv, index=False)\n",
    "    print(f\"[{SPLIT}] Saved per-image stats → {stats_csv}\")\n",
    "\n",
    "    # Summary per class\n",
    "    summary = (\n",
    "        stats_df\n",
    "        .groupby(\"class\")\n",
    "        .agg(gray_mean_mean=(\"gray_mean\",\"mean\"),\n",
    "             gray_mean_p10 =(\"gray_mean\", lambda s: pct(s,10)),\n",
    "             gray_mean_p90 =(\"gray_mean\", lambda s: pct(s,90)),\n",
    "             gray_std_mean =(\"gray_std\",\"mean\"),\n",
    "             gray_std_p10  =(\"gray_std\",  lambda s: pct(s,10)),\n",
    "             gray_std_p90  =(\"gray_std\",  lambda s: pct(s,90)),\n",
    "             r_mean=(\"rgb_mean_r\",\"mean\"),\n",
    "             g_mean=(\"rgb_mean_g\",\"mean\"),\n",
    "             b_mean=(\"rgb_mean_b\",\"mean\"))\n",
    "        .round(2)\n",
    "    )\n",
    "    summary_csv = RESULTS / f\"{SPLIT}_photometric_summary.csv\"\n",
    "    summary.to_csv(summary_csv)\n",
    "    print(f\"[{SPLIT}] Saved summary → {summary_csv}\\n\")\n",
    "    print(summary)\n",
    "\n",
    "    # Histograms — grayscale mean and std by class\n",
    "    plt.figure(figsize=(6,4))\n",
    "    for cls in [\"Parasitized\", \"Uninfected\"]:\n",
    "        plt.hist(stats_df.loc[stats_df[\"class\"]==cls, \"gray_mean\"], bins=40, alpha=0.6, label=cls)\n",
    "    plt.xlabel(\"Grayscale mean (0–255)\"); plt.ylabel(\"Count\"); plt.title(f\"{SPLIT.capitalize()} — grayscale mean by class\")\n",
    "    plt.legend(); plt.tight_layout()\n",
    "    out = RESULTS / f\"{SPLIT}_hist_gray_mean_by_class.png\"\n",
    "    plt.savefig(out, dpi=150); plt.close()\n",
    "    print(f\"[{SPLIT}] Saved {out}\")\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    for cls in [\"Parasitized\", \"Uninfected\"]:\n",
    "        plt.hist(stats_df.loc[stats_df[\"class\"]==cls, \"gray_std\"], bins=40, alpha=0.6, label=cls)\n",
    "    plt.xlabel(\"Grayscale std (contrast proxy)\"); plt.ylabel(\"Count\"); plt.title(f\"{SPLIT.capitalize()} — grayscale std by class\")\n",
    "    plt.legend(); plt.tight_layout()\n",
    "    out = RESULTS / f\"{SPLIT}_hist_gray_std_by_class.png\"\n",
    "    plt.savefig(out, dpi=150); plt.close()\n",
    "    print(f\"[{SPLIT}] Saved {out}\")\n",
    "\n",
    "    # Histograms — per-channel means (both classes together)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(stats_df[\"rgb_mean_r\"], bins=40, alpha=0.6, label=\"R\")\n",
    "    plt.hist(stats_df[\"rgb_mean_g\"], bins=40, alpha=0.6, label=\"G\")\n",
    "    plt.hist(stats_df[\"rgb_mean_b\"], bins=40, alpha=0.6, label=\"B\")\n",
    "    plt.xlabel(\"Channel mean (0–255)\"); plt.ylabel(\"Count\"); plt.title(f\"{SPLIT.capitalize()} — channel means (R,G,B)\")\n",
    "    plt.legend(); plt.tight_layout()\n",
    "    out = RESULTS / f\"{SPLIT}_hist_rgb_channel_means.png\"\n",
    "    plt.savefig(out, dpi=150); plt.close()\n",
    "    print(f\"[{SPLIT}] Saved {out}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PZ1GtALrxko"
   },
   "source": [
    "# Step 11 — PyTorch Datasets and Dataloaders\n",
    "\n",
    "This cell defines the training input pipeline and produces a quick visual check.\n",
    "\n",
    "- **Preprocessing.**  \n",
    "  `border_ring_mode_rgb()` estimates the modal border colour from a thin outer ring; `PadResizeBicubic` pads each image to a square with that colour, then resizes to 128×128 using bicubic interpolation.\n",
    "\n",
    "- **Dataset class.**  \n",
    "  `MalariaDataset` reads from a split-specific `DataFrame`, applies the pad+resize transform, and (optionally) mild augmentation: horizontal/vertical flips, ±10° rotation, and brightness/contrast jitter in ~[0.8, 1.2].  \n",
    "  Labels are mapped to integers: `Parasitized→1`, `Uninfected→0`.  \n",
    "  Images are returned as tensors in `[0,1]` (no normalisation here).\n",
    "\n",
    "- **Splits and loaders.**  \n",
    "  Builds `train/val/test` datasets from `splits.csv`, then `DataLoader`s with `batch_size=64`.  \n",
    "  Training uses `shuffle=True` and `drop_last=True` (stable batch shapes); validation/test are sequential.\n",
    "\n",
    "- **Preview.**  \n",
    "  Grabs one training batch, makes a 32-image grid, and saves it to `results/loader_preview_train.png`. The preview reflects pad+resize and augmentations; tensors are unnormalised.\n",
    "\n",
    "This establishes a consistent, reproducible input path for later model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28374,
     "status": "ok",
     "timestamp": 1761380959842,
     "user": {
      "displayName": "Simon Britzelli",
      "userId": "06415475213358217915"
     },
     "user_tz": -120
    },
    "id": "pjpJ9sf9rzZk",
    "outputId": "0460ebc1-6061-406e-a276-2754aaaf0ef8"
   },
   "outputs": [],
   "source": [
    "# Step 11 — PyTorch datasets & dataloaders (canonical preprocessing; unnormalised tensors)\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.utils as vutils\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RESULTS = RESULTS_DIR\n",
    "splits_df = pd.read_csv(RESULTS / \"splits.csv\")\n",
    "\n",
    "# --- Canonical pad→resize used by the dataset ---\n",
    "NEAR_BLACK_THR = 10\n",
    "MIN_MODE_FRAC  = 0.60\n",
    "\n",
    "def pad_resize_canonical(img: Image.Image, target_size: int = 128, ring: int = 2) -> Image.Image:\n",
    "    img = img.convert(\"RGB\")\n",
    "    arr = np.array(img)\n",
    "    mode_rgb, frac = _ring_mode_from_array(arr, ring=ring)\n",
    "    near_black = all(c <= NEAR_BLACK_THR for c in mode_rgb)\n",
    "    use_constant = (near_black and frac >= MIN_MODE_FRAC)\n",
    "\n",
    "    w, h = img.size\n",
    "    if w != h:\n",
    "        side = max(w, h)\n",
    "        pl = (side - w) // 2; pr = side - w - pl\n",
    "        pt = (side - h) // 2; pb = side - h - pt\n",
    "        if use_constant:\n",
    "            padded = ImageOps.expand(img, border=(pl, pt, pr, pb), fill=mode_rgb)\n",
    "        else:\n",
    "            pad_width = ((pt, pb), (pl, pr), (0, 0))\n",
    "            padded = Image.fromarray(np.pad(arr, pad_width, mode=\"reflect\"))\n",
    "    else:\n",
    "        padded = img\n",
    "\n",
    "    return padded.resize((target_size, target_size), resample=Image.BICUBIC)\n",
    "\n",
    "class PadResizeCanonical:\n",
    "    \"\"\"PIL → canonical pad→resize to size×size; used inside the dataset.\"\"\"\n",
    "    def __init__(self, size: int = 128, ring: int = 2):\n",
    "        self.size = size\n",
    "        self.ring = ring\n",
    "    def __call__(self, img: Image.Image) -> Image.Image:\n",
    "        return pad_resize_canonical(img, target_size=self.size, ring=self.ring)\n",
    "\n",
    "class MalariaDataset(Dataset):\n",
    "    \"\"\"Single-cell crops with light geometric/photometric augments for training.\"\"\"\n",
    "    def __init__(self, frame: pd.DataFrame, split: str, augment: bool):\n",
    "        self.df = frame.reset_index(drop=True)\n",
    "        self.split = split\n",
    "        self.augment = augment\n",
    "        self.pad_resize = PadResizeCanonical(size=128, ring=2)\n",
    "\n",
    "        # Augmentations (keep mild to preserve morphology)\n",
    "        self.pil_aug = T.Compose([\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.RandomVerticalFlip(p=0.5),\n",
    "            T.RandomRotation(degrees=10, fill=0),\n",
    "            T.ColorJitter(brightness=0.2, contrast=0.2)  # ≈ scales [0.8, 1.2]\n",
    "        ])\n",
    "\n",
    "        self.to_tensor = T.ToTensor()  # maps to [0,1] float32 (C,H,W)\n",
    "        self.label_map = {\"Parasitized\": 1, \"Uninfected\": 0}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        with Image.open(row[\"path\"]) as img:\n",
    "            img = pad_resize_canonical(img, target_size=128, ring=2)\n",
    "        if self.augment:\n",
    "            img = self.pil_aug(img)\n",
    "        x = self.to_tensor(img)\n",
    "        y = torch.tensor(self.label_map[row[\"class\"]], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "# Build splits\n",
    "train_df = splits_df[splits_df[\"split\"]==\"train\"]\n",
    "val_df   = splits_df[splits_df[\"split\"]==\"val\"]\n",
    "test_df  = splits_df[splits_df[\"split\"]==\"test\"]\n",
    "\n",
    "train_set = MalariaDataset(train_df, split=\"train\", augment=True)\n",
    "val_set   = MalariaDataset(val_df,   split=\"val\",   augment=False)\n",
    "test_set  = MalariaDataset(test_df,  split=\"test\",  augment=False)\n",
    "\n",
    "BATCH_SIZE  = 64\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY  = False\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, drop_last=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "test_loader  = DataLoader(test_set,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "print(f\"Train/Val/Test sizes: {len(train_set)} / {len(val_set)} / {len(test_set)}\")\n",
    "\n",
    "# Preview a batch from the train loader and save a grid\n",
    "torch.manual_seed(123)\n",
    "xb, yb = next(iter(train_loader))  # xb: [B,3,128,128]\n",
    "grid = vutils.make_grid(xb[:32], nrow=8, padding=2)\n",
    "out_path = RESULTS / \"loader_preview_train.png\"\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Train preview (after canonical pad+resize; unnormalised)\")\n",
    "plt.imshow(np.transpose(grid.numpy(), (1,2,0)))\n",
    "plt.savefig(out_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"Saved preview grid → {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYfCX_hZuWde"
   },
   "source": [
    "# Step 12 — Add ImageNet Normalisation and Rebuild Dataloaders\n",
    "\n",
    "This cell introduces ImageNet normalisation to match the MobileNetV2 pretraining stats and rebuilds the loaders.\n",
    "\n",
    "- **Normalisation.**  \n",
    "  Uses `IMAGENET_MEAN=[0.485, 0.456, 0.406]` and `IMAGENET_STD=[0.229, 0.224, 0.225]`. Applied after `ToTensor()` in the dataset; keeps transforms consistent with the pretrained backbone.\n",
    "\n",
    "- **Preprocessing and augments.**  \n",
    "  Same pad-to-square + bicubic resize as before; light PIL-level augmentation on the training set only (H/V flip, ±10° rotation with `fill=0`, brightness and contrast jitter ~[0.8, 1.2]). Validation and test are deterministic.\n",
    "\n",
    "- **Dataset class change.**  \n",
    "  `MalariaDataset(..., normalise=True)` adds `T.Normalize(mean, std)` to the pipeline; set `normalise=False` to skip it if needed.\n",
    "\n",
    "- **Rebuilt loaders.**  \n",
    "  Constructs `train/val/test` dataloaders with `batch_size=64`; `shuffle=True` and `drop_last=True` for training; sequential evaluation for val/test.\n",
    "\n",
    "- **Preview and display note.**  \n",
    "  Saves a normalised-batch preview to `results/loader_preview_train_normalised.png`. The code denormalises the grid before plotting; otherwise images would appear low contrast due to zero-mean scaling.\n",
    "\n",
    "This step aligns input distributions with the pretrained model; it also preserves the earlier augmentation and padding behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17601,
     "status": "ok",
     "timestamp": 1761380977444,
     "user": {
      "displayName": "Simon Britzelli",
      "userId": "06415475213358217915"
     },
     "user_tz": -120
    },
    "id": "f1NGl8G9uYGI",
    "outputId": "6ca32218-7a05-4a0b-d636-54009adb7ba7"
   },
   "outputs": [],
   "source": [
    "# Step 12 — Add ImageNet normalisation and rebuild dataloaders (canonical preprocessing)\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.utils as vutils\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RESULTS = RESULTS_DIR\n",
    "splits_df = pd.read_csv(RESULTS / \"splits.csv\")\n",
    "\n",
    "# ImageNet normalisation constants\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "# --- Canonical pad→resize used by the dataset ---\n",
    "NEAR_BLACK_THR = 10\n",
    "MIN_MODE_FRAC  = 0.60\n",
    "\n",
    "def _ring_mode_from_array(arr: np.ndarray, ring: int = 2, max_pixels: int = 20000):\n",
    "    h, w, _ = arr.shape\n",
    "    top = arr[0:ring]; bottom = arr[h-ring:h]\n",
    "    left = arr[:, 0:ring]; right = arr[:, w-ring:w]\n",
    "    ring_pixels = np.concatenate([top.reshape(-1,3), bottom.reshape(-1,3),\n",
    "                                  left.reshape(-1,3), right.reshape(-1,3)], axis=0)\n",
    "    if ring_pixels.shape[0] > max_pixels:\n",
    "        idx = np.random.choice(ring_pixels.shape[0], size=max_pixels, replace=False)\n",
    "        ring_pixels = ring_pixels[idx]\n",
    "    cnt = Counter([tuple(int(x) for x in v) for v in ring_pixels])\n",
    "    mode_rgb, n = cnt.most_common(1)[0]\n",
    "    return mode_rgb, n / max(1, ring_pixels.shape[0])\n",
    "\n",
    "class PadResizeCanonical:\n",
    "    def __init__(self, size: int = 128, ring: int = 2):\n",
    "        self.size = size\n",
    "        self.ring = ring\n",
    "    def __call__(self, img: Image.Image) -> Image.Image:\n",
    "        return pad_resize_canonical(img, target_size=self.size, ring=self.ring)\n",
    "\n",
    "class MalariaDataset(Dataset):\n",
    "    \"\"\"Single-cell crops with optional augmentation and ImageNet normalisation.\"\"\"\n",
    "    def __init__(self, frame: pd.DataFrame, split: str, augment: bool, normalise: bool = True):\n",
    "        self.df = frame.reset_index(drop=True)\n",
    "        self.split = split\n",
    "        self.augment = augment\n",
    "        self.pad_resize = PadResizeCanonical(size=128, ring=2)\n",
    "\n",
    "        self.pil_aug = T.Compose([\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.RandomVerticalFlip(p=0.5),\n",
    "            T.RandomRotation(degrees=10, fill=0),\n",
    "            T.ColorJitter(brightness=0.2, contrast=0.2)\n",
    "        ])\n",
    "\n",
    "        self.to_tensor = T.ToTensor()\n",
    "        self.normalise = T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD) if normalise else None\n",
    "        self.label_map = {\"Parasitized\": 1, \"Uninfected\": 0}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        with Image.open(row[\"path\"]) as img:\n",
    "            img = pad_resize_canonical(img, target_size=128, ring=2)\n",
    "        if self.augment:\n",
    "            img = self.pil_aug(img)\n",
    "        x = self.to_tensor(img)\n",
    "        if self.normalise is not None:\n",
    "            x = self.normalise(x)\n",
    "        y = torch.tensor(self.label_map[row[\"class\"]], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "# Rebuild loaders\n",
    "train_df = splits_df[splits_df[\"split\"]==\"train\"]\n",
    "val_df   = splits_df[splits_df[\"split\"]==\"val\"]\n",
    "test_df  = splits_df[splits_df[\"split\"]==\"test\"]\n",
    "\n",
    "train_set = MalariaDataset(train_df, split=\"train\", augment=True,  normalise=True)\n",
    "val_set   = MalariaDataset(val_df,   split=\"val\",   augment=False, normalise=True)\n",
    "test_set  = MalariaDataset(test_df,  split=\"test\",  augment=False, normalise=True)\n",
    "\n",
    "BATCH_SIZE  = 64\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY  = False\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, drop_last=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "test_loader  = DataLoader(test_set,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "print(f\"Rebuilt loaders — sizes: {len(train_set)} / {len(val_set)} / {len(test_set)}\")\n",
    "\n",
    "# Preview (denormalised for display)\n",
    "xb, yb = next(iter(train_loader))\n",
    "grid = vutils.make_grid(xb[:32].cpu(), nrow=8, padding=2)\n",
    "\n",
    "mean = torch.tensor(IMAGENET_MEAN).view(3,1,1)\n",
    "std  = torch.tensor(IMAGENET_STD).view(3,1,1)\n",
    "grid_disp = grid * std + mean\n",
    "\n",
    "plt.figure(figsize=(10,5)); plt.axis(\"off\")\n",
    "plt.title(\"Train preview (canonical pad+resize; ImageNet-normalised tensors shown denormalised)\")\n",
    "plt.imshow(np.transpose(grid_disp.numpy(), (1,2,0)))\n",
    "out_path = RESULTS / \"loader_preview_train_normalised.png\"\n",
    "plt.savefig(out_path, dpi=150, bbox_inches='tight'); plt.close()\n",
    "print(f\"Saved normalised preview grid → {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uH-xlvZueDt"
   },
   "source": [
    "# Step 13 — Model Definition, Loss, Optimiser, and Quick Eval\n",
    "\n",
    "This cell instantiates the baseline classifier, sets up training primitives, and verifies the pipeline with a single optimisation step.\n",
    "\n",
    "- **Backbone and head.**  \n",
    "  Builds MobileNetV2 with ImageNet weights; replaces the classifier with a single-linear layer producing one **logit** (binary task). Moves the model to CPU/GPU; freezes all feature layers; keeps only the classifier trainable.\n",
    "\n",
    "- **Loss with label smoothing.**  \n",
    "  Uses `BCEWithLogitsLoss` on logits; applies manual label smoothing (`ε=0.05`) so targets become `t*(1−ε)+0.5ε`. This can stabilise early training and reduce overconfident predictions.\n",
    "\n",
    "- **Optimiser.**  \n",
    "  Adam with `lr=1e-3`, restricted to trainable parameters (the head).\n",
    "\n",
    "- **Evaluation helper.**  \n",
    "  `evaluate_auc()` switches to eval mode; gathers sigmoid probabilities; returns ROC–AUC and accuracy at a fixed 0.5 threshold.\n",
    "\n",
    "- **Sanity pass.**  \n",
    "  Runs a single forward/backward/step on one training batch to confirm the end-to-end path; prints the loss.\n",
    "\n",
    "- **Baseline metrics and checkpoint.**  \n",
    "  Reports pre-training validation metrics (AUC often ~0.5–0.6 at this stage) and saves the initial model state to `results/mobilenetv2_baseline_init.pt`.\n",
    "\n",
    "This prepares a clean head-only warm-up in the next step while keeping the pretrained feature extractor frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 777523,
     "status": "ok",
     "timestamp": 1761381754961,
     "user": {
      "displayName": "Simon Britzelli",
      "userId": "06415475213358217915"
     },
     "user_tz": -120
    },
    "id": "70RWOPR4ufjU",
    "outputId": "3cc21423-cb40-48b1-f9c0-4b5fed0506df"
   },
   "outputs": [],
   "source": [
    "# Step 13 — Model, loss, optimiser, and eval utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "RESULTS = RESULTS_DIR\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# --- Build MobileNetV2 and replace classifier ---\n",
    "def build_mobilenetv2_single_logit(pretrained: bool = True):\n",
    "    try:\n",
    "        # torchvision >= 0.13 style\n",
    "        weights = models.MobileNet_V2_Weights.DEFAULT if pretrained else None\n",
    "        net = models.mobilenet_v2(weights=weights)\n",
    "    except Exception:\n",
    "        # Fallback (older API or offline)\n",
    "        net = models.mobilenet_v2(pretrained=pretrained)\n",
    "    in_features = net.classifier[1].in_features\n",
    "    net.classifier[1] = nn.Linear(in_features, 1)  # single logit\n",
    "    return net\n",
    "\n",
    "model = build_mobilenetv2_single_logit(pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Freeze backbone (all except classifier)\n",
    "for name, p in model.features.named_parameters():\n",
    "    p.requires_grad = False\n",
    "for p in model.classifier.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# --- Loss with manual label smoothing for binary ---\n",
    "LABEL_SMOOTH = 0.05\n",
    "bce_logits = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "def bce_with_logits_smooth(logits, targets, eps=LABEL_SMOOTH):\n",
    "    # targets: LongTensor {0,1} -> float\n",
    "    t = targets.float()\n",
    "    t_smooth = t*(1.0 - eps) + 0.5*eps\n",
    "    return bce_logits(logits.view(-1), t_smooth)\n",
    "\n",
    "# --- Optimiser ---\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "\n",
    "# --- Metrics ---\n",
    "@torch.no_grad()\n",
    "def evaluate_auc(loader, model):\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    ys = []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        logits = model(xb).view(-1)\n",
    "        p = torch.sigmoid(logits)\n",
    "        probs.append(p.cpu().numpy())\n",
    "        ys.append(yb.cpu().numpy())\n",
    "    probs = np.concatenate(probs)\n",
    "    ys = np.concatenate(ys)\n",
    "    try:\n",
    "        auc = roc_auc_score(ys, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    acc = accuracy_score(ys, (probs >= 0.5).astype(np.int64))\n",
    "    return {\"auc\": auc, \"acc@0.5\": acc}\n",
    "\n",
    "# --- Sanity check: one mini-step ---\n",
    "model.train()\n",
    "xb, yb = next(iter(train_loader))\n",
    "xb = xb.to(device); yb = yb.to(device)\n",
    "logits = model(xb)\n",
    "loss = bce_with_logits_smooth(logits, yb)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Sanity mini-step OK — loss={loss.item():.4f}\")\n",
    "\n",
    "# Pre-training validation AUC (expect ~0.5–0.6)\n",
    "metrics = evaluate_auc(val_loader, model)\n",
    "print(\"Pre-train val metrics:\", metrics)\n",
    "\n",
    "# Save initial (head-warm) weights\n",
    "torch.save(model.state_dict(), RESULTS / \"mobilenetv2_baseline_init.pt\")\n",
    "print(\"Saved initial model weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLTgK4G0Sm1_"
   },
   "source": [
    "# Step 14 — Head Warm-up (frozen backbone) with Early Stopping\n",
    "\n",
    "This cell trains only the classifier head while the MobileNetV2 feature extractor stays frozen.\n",
    "\n",
    "- **Device & seeding.**  \n",
    "  Selects CUDA if available; fixes RNG seeds for Python, NumPy, and Torch; sets deterministic CuDNN (may slow training slightly).\n",
    "\n",
    "- **Preconditions.**  \n",
    "  Verifies that the model, dataloaders, and loss function were created in earlier steps.\n",
    "\n",
    "- **Trainable parameters.**  \n",
    "  Freezes all `model.features` parameters; leaves `model.classifier` trainable.\n",
    "\n",
    "- **Optimiser.**  \n",
    "  Adam with `lr=1e-4` and `weight_decay=1e-4`; scoped to trainable parameters only.\n",
    "\n",
    "- **Mixed precision.**  \n",
    "  Enables autocast + GradScaler on CUDA; reduces memory use; speeds up math; disabled on CPU.\n",
    "\n",
    "- **Epoch routine (`train_one_epoch`).**  \n",
    "  Loops over batches; runs forward under autocast; computes smoothed BCE-with-logits; scales and steps the optimiser; returns mean loss.\n",
    "\n",
    "- **Validation metrics (`evaluate_auc_acc`).**  \n",
    "  Switches to eval mode; collects sigmoid probabilities; reports ROC–AUC and accuracy at 0.5.\n",
    "\n",
    "- **Early stopping on AUC.**  \n",
    "  Trains up to 50 epochs; tracks the best validation AUC; stops after 5 epochs without improvement; keeps the best state dict in memory.\n",
    "\n",
    "- **Outputs & artefacts.**  \n",
    "  Prints timing and best val AUC; saves best head-only weights to  \n",
    "  `results/mobilenetv2_head_warmup.pt`; writes a CSV log `results/trainlog_head.csv`; exports two PNGs with training loss and validation AUC curves:\n",
    "  - `results/curves_head_warmup_loss.png`  \n",
    "  - `results/curves_head_warmup_auc.png`\n",
    "\n",
    "This produces a warmed-up classifier head that is ready for partial fine-tuning of the backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UDxMkku7SprO",
    "outputId": "280125d4-d91d-4344-dc33-6d444a2ad35b"
   },
   "outputs": [],
   "source": [
    "# Step 14 — Warm-up the classifier head (frozen backbone) with early stopping\n",
    "\n",
    "import os, time, math, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- Paths & device ----\n",
    "RESULTS = RESULTS_DIR\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---- Seed (again, safe) ----\n",
    "def seed_everything(seed=42):\n",
    "    import numpy as np, random, torch\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_everything(42)\n",
    "\n",
    "# ---- Expect these from earlier steps ----\n",
    "assert 'model' in globals(), \"Model not found. Please run Step 13 first.\"\n",
    "assert 'train_loader' in globals() and 'val_loader' in globals(), \"Loaders not found. Please run Steps 12–13.\"\n",
    "assert 'bce_with_logits_smooth' in globals(), \"Loss fn not found. Please run Step 13.\"\n",
    "\n",
    "# Ensure backbone frozen, head trainable\n",
    "for p in model.features.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in model.classifier.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Fresh optimiser for head-only training\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                       lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Mixed precision if CUDA available\n",
    "use_amp = (device.type == 'cuda')\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=use_amp)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    n = 0\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "            logits = model(xb).view(-1)\n",
    "            loss = bce_with_logits_smooth(logits, yb)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        bs = xb.size(0)\n",
    "        running += loss.item() * bs\n",
    "        n += bs\n",
    "    return running / max(1, n)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_auc_acc(model, loader, device):\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    ys = []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        logits = model(xb).view(-1)\n",
    "        p = torch.sigmoid(logits)\n",
    "        probs.append(p.cpu().numpy())\n",
    "        ys.append(yb.cpu().numpy())\n",
    "    probs = np.concatenate(probs)\n",
    "    ys = np.concatenate(ys)\n",
    "    auc = roc_auc_score(ys, probs) if (len(np.unique(ys)) > 1) else float('nan')\n",
    "    acc = accuracy_score(ys, (probs >= 0.5).astype(np.int64))\n",
    "    return auc, acc\n",
    "\n",
    "# ---- Training loop with early stopping on val AUC ----\n",
    "MAX_EPOCHS = 50\n",
    "PATIENCE = 5\n",
    "\n",
    "best_auc = -1.0\n",
    "best_state = None\n",
    "no_improve = 0\n",
    "\n",
    "history = []\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, MAX_EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, scaler, device)\n",
    "    val_auc, val_acc = evaluate_auc_acc(model, val_loader, device)\n",
    "\n",
    "    history.append({\"epoch\": epoch, \"train_loss\": train_loss,\n",
    "                    \"val_auc\": float(val_auc), \"val_acc\": float(val_acc)})\n",
    "    print(f\"[Head {epoch:02d}/{MAX_EPOCHS}] \"\n",
    "          f\"train_loss={train_loss:.4f}  val_auc={val_auc:.4f}  val_acc@0.5={val_acc:.4f}  \"\n",
    "          f\"({time.time()-t0:.1f}s)\")\n",
    "\n",
    "    # Early stopping on val AUC\n",
    "    if val_auc > best_auc:\n",
    "        best_auc = val_auc\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(f\"Early stopping triggered (no val AUC improvement in {PATIENCE} epochs).\")\n",
    "            break\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"Head warm-up finished in {elapsed/60:.1f} min. Best val AUC = {best_auc:.4f}\")\n",
    "\n",
    "# ---- Save best checkpoint, log, and curves ----\n",
    "if best_state is not None:\n",
    "    ckpt_path = RESULTS / \"mobilenetv2_head_warmup.pt\"\n",
    "    torch.save(best_state, ckpt_path)\n",
    "    print(f\"Saved best head-only weights → {ckpt_path}\")\n",
    "\n",
    "log_df = pd.DataFrame(history)\n",
    "log_csv = RESULTS / \"trainlog_head.csv\"\n",
    "log_df.to_csv(log_csv, index=False)\n",
    "print(f\"Saved training log → {log_csv}\")\n",
    "\n",
    "# Curves\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"train_loss\"], label=\"train loss\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.title(\"Head warm-up — training loss\"); plt.legend()\n",
    "plt.tight_layout(); plt.savefig(RESULTS / \"curves_head_warmup_loss.png\", dpi=150); plt.close()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"val_auc\"], label=\"val AUC\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"AUC\"); plt.ylim(0.4, 1.0)\n",
    "plt.title(\"Head warm-up — validation AUC\"); plt.legend()\n",
    "plt.tight_layout(); plt.savefig(RESULTS / \"curves_head_warmup_auc.png\", dpi=150); plt.close()\n",
    "\n",
    "print(\"Saved curves →\",\n",
    "      RESULTS / \"curves_head_warmup_loss.png\",\n",
    "      \"and\",\n",
    "      RESULTS / \"curves_head_warmup_auc.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiQb62ZoxH6K"
   },
   "source": [
    "# Step 15 — Fine-tune the last *N* MobileNetV2 blocks (+ head) with early stopping\n",
    "\n",
    "This cell adapts the tail of MobileNetV2 while keeping earlier layers frozen, using mixed precision and differential learning rates.\n",
    "\n",
    "- **Preconditions & device.**  \n",
    "  Requires the model, dataloaders, and smoothed BCE loss from earlier steps. Selects CUDA if available and enables AMP on GPU.\n",
    "\n",
    "- **Warm-up checkpoint load.**  \n",
    "  Restores the best head-only weights from Step 14 (`results/mobilenetv2_head_warmup.pt`) to start fine-tuning from a stable classifier.\n",
    "\n",
    "- **Unfreezing policy.**  \n",
    "  Freezes all feature blocks, then unfreezes the last `N_TAIL=12` blocks (`features[start_idx:]]`) plus the classifier:\n",
    "  - `L = len(model.features)`; `start_idx = max(0, L - N_TAIL)`.\n",
    "  - Prints indices and counts of trainable parameters for transparency.\n",
    "\n",
    "- **Optimiser (differential LR).**  \n",
    "  AdamW with `weight_decay=1e-4`, using a lower LR for the backbone tail and a higher LR for the head:\n",
    "  - Tail (unfrozen feature blocks): `lr=1e-4`  \n",
    "  - Head (classifier): `lr=5e-4`\n",
    "\n",
    "- **Training loop (AMP).**  \n",
    "  - `train_one_epoch_ft`: autocast + GradScaler; computes smoothed BCE-with-logits; aggregates mean loss.  \n",
    "  - `eval_auc_acc`: collects calibrated probabilities (sigmoid) and computes ROC–AUC and accuracy@0.5.\n",
    "\n",
    "- **Early stopping on validation AUC.**  \n",
    "  Trains up to `MAX_EPOCHS=24`; stops after `PATIENCE=3` epochs without AUC improvement. The best-performing state dict is kept in memory.\n",
    "\n",
    "- **Outputs & artefacts.**  \n",
    "  - Best fine-tuned weights: `results/mobilenetv2_finetune_tail.pt`  \n",
    "  - CSV training log: `results/trainlog_finetune_tail.csv`  \n",
    "  - Curves:  \n",
    "    - `results/curves_finetune_tail_loss.png` (training loss)  \n",
    "    - `results/curves_finetune_tail_auc.png` (validation AUC)\n",
    "\n",
    "- **What to adjust later.**  \n",
    "  - `N_TAIL` to widen/narrow the adaptation window if validation performance or calibration needs change.  \n",
    "  - Learning rates if convergence is unstable or too slow.\n",
    "\n",
    "This produces a partially fine-tuned backbone consistent with the project plan: a lightweight model adapted to the dataset while limiting overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2fihVdgxN5F"
   },
   "outputs": [],
   "source": [
    "# Step 15 — Fine-tune the last N MobileNetV2 blocks (+ head) with early stopping on val AUC\n",
    "\n",
    "import time, numpy as np, torch, torch.nn as nn, torch.optim as optim\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Paths & device ---\n",
    "RESULTS = RESULTS_DIR\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_amp = (device.type == 'cuda')\n",
    "\n",
    "# --- Safety: expect these from earlier steps ---\n",
    "assert 'model' in globals(), \"Model not found. Please run Step 13 first.\"\n",
    "assert 'train_loader' in globals() and 'val_loader' in globals(), \"Loaders not found. Run Steps 11–12.\"\n",
    "assert 'bce_with_logits_smooth' in globals(), \"Loss fn not found. Run Step 13.\"\n",
    "\n",
    "# --- Load best head-only checkpoint from Step 14 ---\n",
    "ckpt_head = RESULTS / \"mobilenetv2_head_warmup.pt\"\n",
    "assert ckpt_head.exists(), f\"Missing {ckpt_head}; run Step 14 first.\"\n",
    "model.load_state_dict(torch.load(ckpt_head, map_location=\"cpu\"))\n",
    "model = model.to(device)\n",
    "\n",
    "# --- Freeze all features, then unfreeze the *tail* (last N blocks) + classifier ---\n",
    "for p in model.features.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "N_TAIL = 12  # <-- try adjusting to a larger adaptation window\n",
    "L = len(model.features)\n",
    "start_idx = max(0, L - N_TAIL)\n",
    "tail_indices = list(range(start_idx, L))\n",
    "for i in tail_indices:\n",
    "    # Some entries (e.g., final ConvBNReLU) are modules with parameters; this works for both IR blocks & final 1x1 conv\n",
    "    for p in model.features[i].parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "for p in model.classifier.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# --- Introspection & transparency ---\n",
    "def count_trainable(module):\n",
    "    return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"MobileNetV2 features length: {L}\")\n",
    "print(f\"Unfreezing feature blocks (0-based): {tail_indices}\")\n",
    "print(f\"Trainable params (tail): {count_trainable(model.features[start_idx:])}\")\n",
    "print(f\"Trainable params (head): {count_trainable(model.classifier)}\")\n",
    "\n",
    "# --- Optimizer: smaller LR for tail, larger LR for head ---\n",
    "backbone_params = [p for p in model.features.parameters() if p.requires_grad]\n",
    "head_params     = list(model.classifier.parameters())\n",
    "optimizer = optim.AdamW([\n",
    "    {\"params\": backbone_params, \"lr\": 1e-4},\n",
    "    {\"params\": head_params,     \"lr\": 5e-4},\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=use_amp)\n",
    "\n",
    "# --- Train / Eval helpers (AMP) ---\n",
    "def train_one_epoch_ft(model, loader, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total, n = 0.0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True); yb = yb.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "            logits = model(xb).view(-1)\n",
    "            loss = bce_with_logits_smooth(logits, yb)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        bs = xb.size(0); total += loss.item() * bs; n += bs\n",
    "    return total / max(1, n)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_auc_acc(model, loader, device):\n",
    "    model.eval()\n",
    "    probs, ys = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True); yb = yb.to(device, non_blocking=True)\n",
    "        p = torch.sigmoid(model(xb).view(-1))\n",
    "        probs.append(p.cpu().numpy()); ys.append(yb.cpu().numpy())\n",
    "    probs = np.concatenate(probs); ys = np.concatenate(ys)\n",
    "    auc = roc_auc_score(ys, probs) if (len(np.unique(ys)) > 1) else float('nan')\n",
    "    acc = accuracy_score(ys, (probs >= 0.5).astype(np.int64))\n",
    "    return auc, acc\n",
    "\n",
    "# --- Training loop with early stopping on val AUC ---\n",
    "MAX_EPOCHS = 24\n",
    "PATIENCE = 3\n",
    "history = []\n",
    "\n",
    "best_auc = -1.0\n",
    "best_state = None\n",
    "no_improve = 0\n",
    "t_start = time.time()\n",
    "\n",
    "for epoch in range(1, MAX_EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "    train_loss = train_one_epoch_ft(model, train_loader, optimizer, scaler, device)\n",
    "    val_auc, val_acc = eval_auc_acc(model, val_loader, device)\n",
    "    history.append({\"epoch\": epoch, \"train_loss\": float(train_loss),\n",
    "                    \"val_auc\": float(val_auc), \"val_acc\": float(val_acc)})\n",
    "    print(f\"[FT {epoch:02d}/{MAX_EPOCHS}] train_loss={train_loss:.4f}  val_auc={val_auc:.4f}  \"\n",
    "          f\"val_acc@0.5={val_acc:.4f}  ({time.time()-t0:.1f}s)\")\n",
    "\n",
    "    if val_auc > best_auc:\n",
    "        best_auc = val_auc\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(f\"Early stopping (no val AUC improvement in {PATIENCE} epochs).\")\n",
    "            break\n",
    "\n",
    "elapsed = time.time() - t_start\n",
    "print(f\"Fine-tune finished in {elapsed/60:.1f} min. Best val AUC = {best_auc:.4f}\")\n",
    "\n",
    "# --- Save best checkpoint, log, and curves ---\n",
    "if best_state is not None:\n",
    "    ckpt = RESULTS / \"mobilenetv2_finetune_tail.pt\"\n",
    "    torch.save(best_state, ckpt)\n",
    "    print(f\"Saved fine-tuned weights → {ckpt}\")\n",
    "\n",
    "log_df = pd.DataFrame(history)\n",
    "log_csv = RESULTS / \"trainlog_finetune_tail.csv\"\n",
    "log_df.to_csv(log_csv, index=False)\n",
    "print(f\"Saved training log → {log_csv}\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"train_loss\"], label=\"train loss\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.title(\"Fine-tune (tail) — training loss\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "plt.savefig(RESULTS / \"curves_finetune_tail_loss.png\", dpi=150); plt.close()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"val_auc\"], label=\"val AUC\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"AUC\"); plt.ylim(0.9, 1.0)\n",
    "plt.title(\"Fine-tune (tail) — validation AUC\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "plt.savefig(RESULTS / \"curves_finetune_tail_auc.png\", dpi=150); plt.close()\n",
    "\n",
    "print(\"Saved curves →\",\n",
    "      RESULTS / \"curves_finetune_tail_loss.png\",\n",
    "      \"and\",\n",
    "      RESULTS / \"curves_finetune_tail_auc.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 16 — Calibration (temperature scaling), reliability, and operating point\n",
    "\n",
    "This cell calibrates predicted probabilities on the **validation** split, evaluates calibration quality, draws a reliability diagram, and selects a clinically oriented decision threshold.\n",
    "\n",
    "- **Model state & device.**  \n",
    "  Ensures the fine-tuned MobileNetV2 weights (`mobilenetv2_finetune_tail.pt`) are loaded and the model is in `eval()` on CPU/GPU.\n",
    "\n",
    "- **Logit collection (validation).**  \n",
    "  `collect_logits_labels` runs the model over `val_loader` to gather raw logits and ground-truth labels; uncalibrated probabilities are `sigmoid(logits)`.\n",
    "\n",
    "- **Temperature scaling (Platt-style for logits).**  \n",
    "  - Defines `TemperatureScaler(logT)` so that calibrated logits = `logits / T` with `T = exp(logT) > 0`.  \n",
    "  - Fits `T` by minimizing **negative log-likelihood** (binary cross-entropy) on validation using **LBFGS**.  \n",
    "  - Produces calibrated probabilities `sigmoid(logits / T)`.  \n",
    "  - Note: **AUC is unchanged** by temperature scaling (rank-preserving).\n",
    "\n",
    "- **Calibration metrics (before/after).**  \n",
    "  Computes on validation:\n",
    "  - **NLL (log loss)** — cross-entropy on probabilities.  \n",
    "  - **Brier score** — mean squared error of probabilities.  \n",
    "  - **ECE (Expected Calibration Error)** — via 10 equal-width probability bins; also emits a per-bin table (`ece_bins_val.csv`).\n",
    "\n",
    "- **Reliability diagram.**  \n",
    "  Plots bin-mean predicted probability vs empirical positive rate for uncalibrated and calibrated models and saves to `reliability_diagram_val.png`.\n",
    "\n",
    "- **Operating point selection (validation, calibrated).**  \n",
    "  Scans candidate thresholds to **maximize sensitivity** subject to **specificity ≥ 0.95** (fallback selects the closest specificity, then best sensitivity).  \n",
    "  Returns the chosen `threshold`, its `val_sensitivity`, and `val_specificity`.\n",
    "\n",
    "- **Persistence of artefacts.**  \n",
    "  - `temperature.txt` — fitted scalar `T`.  \n",
    "  - `operating_point_val.json` — JSON payload with `temperature_T`, selected operating point (mode, target specificity, threshold, achieved sens/spec), reference calibration metrics, and file paths to artefacts.  \n",
    "  - `ece_bins_val.csv`, `reliability_diagram_val.png`.\n",
    "\n",
    "- **Why this matters.**  \n",
    "  Temperature scaling improves **probability calibration** without affecting ranking (AUC). Selecting a threshold at high specificity supports conservative screening settings; the same `T` and threshold will be reused on the **test** set and in downstream analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 16 — Calibration (temperature scaling) + reliability diagram + operating point\n",
    "import os, json, math, numpy as np, pandas as pd\n",
    "import torch, torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "\n",
    "# ---- Paths, device, and preconditions ----\n",
    "RESULTS = RESULTS_DIR  # from earlier steps\n",
    "device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "assert 'model' in globals(), \"Model not found – please run Steps 13–15.\"\n",
    "assert 'val_loader' in globals(), \"val_loader not found – please run Steps 11–12.\"\n",
    "\n",
    "# If the current model isn't the fine-tuned one, (re)load the best FT checkpoint\n",
    "ckpt_ft = RESULTS / \"mobilenetv2_finetune_tail.pt\"\n",
    "if ckpt_ft.exists():\n",
    "    try:\n",
    "        state = torch.load(ckpt_ft, map_location=\"cpu\", weights_only=True)\n",
    "    except TypeError:  # older PyTorch\n",
    "        state = torch.load(ckpt_ft, map_location=\"cpu\")\n",
    "    model.load_state_dict(state)\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# ---- Collect validation logits & labels (uncalibrated) ----\n",
    "@torch.no_grad()\n",
    "def collect_logits_labels(loader, model, device):\n",
    "    model.eval()\n",
    "    logits_list, y_list = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        out = model(xb).view(-1)  # logits\n",
    "        logits_list.append(out.detach().cpu())\n",
    "        y_list.append(yb.detach().cpu())\n",
    "    logits = torch.cat(logits_list).float()\n",
    "    y      = torch.cat(y_list).long().numpy()\n",
    "    return logits, y\n",
    "\n",
    "val_logits, val_y = collect_logits_labels(val_loader, model, device)\n",
    "val_p_uncal = torch.sigmoid(val_logits).numpy()\n",
    "\n",
    "# ---- Temperature scaling (fit T on validation by NLL) ----\n",
    "class TemperatureScaler(nn.Module):\n",
    "    def __init__(self, init_T=1.0):\n",
    "        super().__init__()\n",
    "        # parameterized as logT for positivity\n",
    "        self.logT = nn.Parameter(torch.tensor(math.log(init_T), dtype=torch.float32))\n",
    "    def forward(self, logits):\n",
    "        T = torch.exp(self.logT)\n",
    "        return logits / T\n",
    "    def T_value(self):\n",
    "        return float(torch.exp(self.logT).detach().cpu().item())\n",
    "\n",
    "def fit_temperature(logits, y, max_iter=100, lr=0.01):\n",
    "    # logits: torch tensor on CPU, y: numpy array {0,1}\n",
    "    y_t = torch.from_numpy(y.astype(np.float32))\n",
    "    scaler = TemperatureScaler(init_T=1.0)\n",
    "    scaler.train()\n",
    "    opt = torch.optim.LBFGS(scaler.parameters(), lr=lr, max_iter=max_iter, line_search_fn=\"strong_wolfe\")\n",
    "\n",
    "    bce = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    def closure():\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits_cal = scaler(logits)\n",
    "        loss = bce(logits_cal.view(-1), y_t)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    opt.step(closure)\n",
    "    return scaler\n",
    "\n",
    "scaler = fit_temperature(val_logits.clone(), val_y, max_iter=200, lr=0.01)\n",
    "T_fitted = scaler.T_value()\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_logits_cal = scaler(val_logits.clone()).view(-1)\n",
    "val_p_cal = torch.sigmoid(val_logits_cal).numpy()\n",
    "\n",
    "# ---- Metrics: AUC, NLL (log loss), Brier ----\n",
    "def brier_score(probs, y):\n",
    "    probs = np.clip(probs, 1e-6, 1-1e-6)\n",
    "    return float(np.mean((probs - y)**2))\n",
    "\n",
    "val_auc_ref = roc_auc_score(val_y, val_p_uncal)  # unchanged by temperature scaling\n",
    "nll_uncal   = log_loss(val_y, np.clip(val_p_uncal, 1e-6, 1-1e-6))\n",
    "nll_cal     = log_loss(val_y, np.clip(val_p_cal,   1e-6, 1-1e-6))\n",
    "brier_uncal = brier_score(val_p_uncal, val_y)\n",
    "brier_cal   = brier_score(val_p_cal,   val_y)\n",
    "\n",
    "# ---- ECE & reliability (binary): prob vs empirical positive rate over p∈[0,1] ----\n",
    "def bin_stats_prob_vs_posrate(probs, y, n_bins=10):\n",
    "    probs = np.clip(probs.astype(np.float64), 1e-6, 1-1e-6)\n",
    "    edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    ids   = np.digitize(probs, edges[1:-1])  # 0..n_bins-1\n",
    "    rows  = []\n",
    "    for b in range(n_bins):\n",
    "        m = (ids == b)\n",
    "        n = int(np.sum(m))\n",
    "        lo, hi = float(edges[b]), float(edges[b+1])\n",
    "        if n == 0:\n",
    "            rows.append(dict(bin=b, n=0, bin_lower=lo, bin_upper=hi,\n",
    "                             avg_prob=np.nan, pos_rate=np.nan))\n",
    "        else:\n",
    "            rows.append(dict(\n",
    "                bin=b, n=n, bin_lower=lo, bin_upper=hi,\n",
    "                avg_prob=float(np.mean(probs[m])),\n",
    "                pos_rate=float(np.mean(y[m]))\n",
    "            ))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def ece_binary_probrate(probs, y, n_bins=10):\n",
    "    df = bin_stats_prob_vs_posrate(probs, y, n_bins=n_bins)\n",
    "    n_tot = df[\"n\"].sum()\n",
    "    if n_tot == 0:\n",
    "        return float(\"nan\"), df\n",
    "    gap = np.abs(df[\"pos_rate\"] - df[\"avg_prob\"])\n",
    "    w   = df[\"n\"] / n_tot\n",
    "    ece = float(np.nansum(gap * w))\n",
    "    return ece, df\n",
    "\n",
    "N_BINS = 10\n",
    "ece_uncal, df_uncal = ece_binary_probrate(val_p_uncal, val_y, n_bins=N_BINS)\n",
    "ece_cal,   df_cal   = ece_binary_probrate(val_p_cal,   val_y, n_bins=N_BINS)\n",
    "\n",
    "# Save bins table (uncal + cal)\n",
    "bins_out = df_uncal.rename(columns={\"avg_prob\":\"uncal_avg_prob\",\"pos_rate\":\"uncal_pos_rate\"})\n",
    "bins_out[[\"cal_avg_prob\",\"cal_pos_rate\"]] = df_cal[[\"avg_prob\",\"pos_rate\"]]\n",
    "bins_csv = RESULTS / \"ece_bins_val.csv\"\n",
    "bins_out.to_csv(bins_csv, index=False)\n",
    "\n",
    "# ---- Reliability diagram (probability vs positive rate) ----\n",
    "def plot_reliability_binary(df_uncal, df_cal, out_path):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    xs = np.linspace(0,1,101)\n",
    "    plt.plot(xs, xs, linestyle=\"--\", linewidth=1, label=\"perfectly calibrated\")\n",
    "    for label, df in [(\"uncalibrated\", df_uncal), (\"calibrated\", df_cal)]:\n",
    "        d = df.dropna(subset=[\"avg_prob\",\"pos_rate\"])\n",
    "        plt.plot(d[\"avg_prob\"], d[\"pos_rate\"], marker=\"o\", linewidth=1, label=label)\n",
    "    plt.xlabel(\"predicted probability (bin mean)\")\n",
    "    plt.ylabel(\"empirical positive rate (bin mean)\")\n",
    "    plt.title(\"Reliability diagram — validation\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "rel_png = RESULTS / \"reliability_diagram_val.png\"\n",
    "plot_reliability_binary(df_uncal, df_cal, rel_png)\n",
    "\n",
    "# ---- Operating point on validation: sensitivity at specificity ≥ 0.95 ----\n",
    "def sens_spec_at_threshold(p, y, thr):\n",
    "    y_pred = (p >= thr).astype(np.int64)\n",
    "    tp = np.sum((y_pred==1) & (y==1))\n",
    "    tn = np.sum((y_pred==0) & (y==0))\n",
    "    fp = np.sum((y_pred==1) & (y==0))\n",
    "    fn = np.sum((y_pred==0) & (y==1))\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "    return sens, spec\n",
    "\n",
    "def pick_threshold_sens_at_spec(p, y, target_spec=0.95):\n",
    "    # scan unique probabilities plus a small grid to be robust\n",
    "    cand = np.unique(np.concatenate([p, np.linspace(0,1,2001)]))\n",
    "    best = dict(threshold=None, sensitivity=-1.0, specificity=float(\"nan\"))\n",
    "    for thr in cand:\n",
    "        sens, spec = sens_spec_at_threshold(p, y, thr)\n",
    "        if np.isnan(spec): \n",
    "            continue\n",
    "        if spec >= target_spec and sens > best[\"sensitivity\"]:\n",
    "            best = dict(threshold=float(thr), sensitivity=float(sens), specificity=float(spec))\n",
    "    # If none meet the spec constraint, fall back to closest by spec then best sens\n",
    "    if best[\"threshold\"] is None:\n",
    "        gaps = []\n",
    "        for thr in cand:\n",
    "            sens, spec = sens_spec_at_threshold(p, y, thr)\n",
    "            if np.isnan(spec): \n",
    "                continue\n",
    "            gaps.append((abs(spec - target_spec), -sens, thr, sens, spec))\n",
    "        if gaps:\n",
    "            gaps.sort()\n",
    "            _, _, thr, sens, spec = gaps[0]\n",
    "            best = dict(threshold=float(thr), sensitivity=float(sens), specificity=float(spec))\n",
    "    return best\n",
    "\n",
    "op = pick_threshold_sens_at_spec(val_p_cal, val_y, target_spec=0.95)\n",
    "\n",
    "# ---- Persist temperature & print a compact summary ----\n",
    "with open(RESULTS / \"temperature.txt\", \"w\") as f:\n",
    "    f.write(f\"{T_fitted:.6f}\\n\")\n",
    "\n",
    "print(f\"Fitted temperature T = {T_fitted:.4f}\")\n",
    "print(f\"Validation AUC (reference): {val_auc_ref:.4f}\")\n",
    "print(f\"NLL  (uncal → cal): {nll_uncal:.4f} → {nll_cal:.4f}\")\n",
    "print(f\"Brier(uncal → cal): {brier_uncal:.4f} → {brier_cal:.4f}\")\n",
    "print(f\"ECE  (uncal → cal): {ece_uncal:.4f} → {ece_cal:.4f}\")\n",
    "print(f\"Reliability diagram saved → {rel_png}\")\n",
    "print(f\"ECE bins CSV saved       → {bins_csv}\")\n",
    "\n",
    "print(\"\\nOperating point (val, calibrated):\")\n",
    "print(\"  mode: sens_at_spec\")\n",
    "print(\"  target_specificity:\", 0.95)\n",
    "print(f\"  threshold: {op['threshold']}\")\n",
    "print(f\"  val_sensitivity: {op['sensitivity']}\")\n",
    "print(f\"  val_specificity: {op['specificity']}\")\n",
    "\n",
    "# ---- Save calibrated operating point & threshold to JSON (robust to 'T' name clash) ----\n",
    "from datetime import datetime\n",
    "import json\n",
    "from numbers import Number\n",
    "\n",
    "def _to_float(x):\n",
    "    \"\"\"Best-effort conversion of tensors/ndarrays/scalars to plain float; return None if impossible.\"\"\"\n",
    "    try:\n",
    "        if x is None:\n",
    "            return None\n",
    "        if isinstance(x, Number):\n",
    "            return float(x)\n",
    "        # torch tensor?\n",
    "        if hasattr(x, \"item\") and callable(getattr(x, \"item\")):\n",
    "            return float(x.item())\n",
    "        # numpy scalar/array?\n",
    "        try:\n",
    "            import numpy as np\n",
    "            if isinstance(x, (np.floating, np.integer)):\n",
    "                return float(x)\n",
    "            if isinstance(x, np.ndarray) and x.size == 1:\n",
    "                return float(x.reshape(()))\n",
    "        except Exception:\n",
    "            pass\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _pick_first_numeric(names):\n",
    "    \"\"\"Return first variable among 'names' that resolves to a numeric float.\"\"\"\n",
    "    g = globals()\n",
    "    l = locals()\n",
    "    for nm in names:\n",
    "        obj = l.get(nm, g.get(nm, None))\n",
    "        val = _to_float(obj)\n",
    "        if val is not None:\n",
    "            return val\n",
    "    return None\n",
    "\n",
    "# 1) Temperature (try the safe alias first if you created it, then other likely names)\n",
    "T_value = _pick_first_numeric([\"T_fitted\", \"temperature_T\", \"T_value\", \"T_fit\", \"temp\", \"temperature\", \"T\"])\n",
    "if T_value is None:\n",
    "    print(\"WARNING: Could not find a numeric temperature; defaulting to 1.0\")\n",
    "    T_value = 1.0\n",
    "\n",
    "# 2) Operating-point fields (prefer dict 'op' if present)\n",
    "op = globals().get(\"op\", {})\n",
    "op_mode = op.get(\"mode\", globals().get(\"op_mode\", \"sens_at_spec\"))\n",
    "\n",
    "target_spec = _to_float(op.get(\"target_specificity\"))\n",
    "if target_spec is None:\n",
    "    target_spec = _to_float(globals().get(\"target_specificity\", 0.95))\n",
    "\n",
    "thr = op.get(\"threshold\", None)\n",
    "if thr is None:\n",
    "    thr = globals().get(\"op_threshold\", globals().get(\"threshold\", None))\n",
    "thr = _to_float(thr)\n",
    "\n",
    "val_sens = _to_float(op.get(\"val_sensitivity\", globals().get(\"val_sensitivity\", None)))\n",
    "val_spec = _to_float(op.get(\"val_specificity\", globals().get(\"val_specificity\", None)))\n",
    "\n",
    "# 3) Optional reference metrics (if you kept them around)\n",
    "val_auc_ref = _to_float(globals().get(\"val_auc_ref\", None))\n",
    "nll_uncal   = _to_float(globals().get(\"nll_uncal\", None))\n",
    "nll_cal     = _to_float(globals().get(\"nll_cal\", None))\n",
    "brier_uncal = _to_float(globals().get(\"brier_uncal\", None))\n",
    "brier_cal   = _to_float(globals().get(\"brier_cal\", None))\n",
    "ece_uncal   = _to_float(globals().get(\"ece_uncal\", None))\n",
    "ece_cal     = _to_float(globals().get(\"ece_cal\", None))\n",
    "\n",
    "summary_json = {\n",
    "    \"temperature_T\": round(T_value, 6),\n",
    "    \"operating_point\": {\n",
    "        \"mode\": op_mode,\n",
    "        \"target_specificity\": round(target_spec, 6) if target_spec is not None else None,\n",
    "        \"threshold\": round(thr, 6) if thr is not None else None,\n",
    "        \"val_sensitivity\": round(val_sens, 6) if val_sens is not None else None,\n",
    "        \"val_specificity\": round(val_spec, 6) if val_spec is not None else None,\n",
    "    },\n",
    "    \"validation_reference\": {\n",
    "        \"auc\": val_auc_ref,\n",
    "        \"nll_uncal\": nll_uncal, \"nll_cal\": nll_cal,\n",
    "        \"brier_uncal\": brier_uncal, \"brier_cal\": brier_cal,\n",
    "        \"ece_uncal\": ece_uncal, \"ece_cal\": ece_cal,\n",
    "    },\n",
    "    \"artifacts\": {\n",
    "        \"reliability_diagram\": str(RESULTS_DIR / \"reliability_diagram_val.png\"),\n",
    "        \"ece_bins_csv\": str(RESULTS_DIR / \"ece_bins_val.csv\"),\n",
    "    },\n",
    "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\")\n",
    "}\n",
    "\n",
    "out_json = RESULTS_DIR / \"operating_point_val.json\"\n",
    "with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary_json, f, indent=2)\n",
    "print(f\"Saved operating-point JSON → {out_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 17 — Final test evaluation (calibrated)\n",
    "\n",
    "This cell evaluates the **final model on the test split**, applying the **temperature** and **decision threshold** obtained on validation, and exports a structured report and plots.\n",
    "\n",
    "- **Inputs & preconditions**\n",
    "  - Expects a MobileNetV2 model in memory and `test_loader` (from Steps 11–15).\n",
    "  - Loads fine-tuned weights `mobilenetv2_finetune_tail.pt` and switches to `eval()` on the selected device (CPU/GPU).\n",
    "  - Loads calibration artifacts from Step 16:\n",
    "    - `temperature_T` (scalar for logit scaling).\n",
    "    - Operating-point metadata from `operating_point_val.json`: `mode`, `target_specificity`, and **validation-derived threshold**.\n",
    "\n",
    "- **Inference & calibration**\n",
    "  - Collects **raw logits** and labels on the test set (`collect_logits_labels`).\n",
    "  - Applies **temperature scaling**: `logits / T` (no re-fitting on test).\n",
    "  - Converts both uncalibrated and calibrated logits to probabilities via `sigmoid`.\n",
    "\n",
    "- **Discrimination (ranking)**\n",
    "  - Computes **AUC** for both uncalibrated and calibrated probabilities (`roc_auc_score`).  \n",
    "    *Note:* Temperature scaling does not change the ranking ideally; any difference reflects numerical or tie effects.\n",
    "\n",
    "- **Calibration quality**\n",
    "  - **NLL (log loss)**, **Brier score**, and **ECE (10-bin)** are computed for uncalibrated and calibrated probabilities.  \n",
    "  - A **reliability diagram** is created for the test set by plotting bin mean predicted probability vs. empirical positive rate.\n",
    "\n",
    "- **Operating point on test (fixed threshold)**\n",
    "  - Uses the **validation-selected threshold** (high-specificity target) and applies it to **calibrated** test probabilities.\n",
    "  - Reports confusion-matrix counts (TP, TN, FP, FN), **sensitivity**, and **specificity** at that threshold.\n",
    "\n",
    "- **Plots & artifacts written**\n",
    "  - `roc_test.png` — ROC curves (uncalibrated vs calibrated).\n",
    "  - `reliability_diagram_test.png` — Test reliability diagram.\n",
    "  - `ece_bins_test.csv` — Bin-level calibration table (uncalibrated + calibrated).\n",
    "  - `test_metrics.json` — Structured summary with: dataset size, temperature, operating-point metrics, discrimination scores, calibration scores, and file paths to artifacts.  \n",
    "    The console also prints a compact summary (AUCs, NLL/Brier/ECE, threshold, sensitivity/specificity, and confusion matrix).\n",
    "\n",
    "- **Why this step**\n",
    "  - Ensures **honest generalization** by reporting all final metrics on held-out test data, using **frozen calibration** and a **fixed clinical operating point** chosen on validation.\n",
    "  - The JSON report and plots facilitate downstream analysis, reproducibility, and inclusion in results sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 17 — Final test evaluation (calibrated)\n",
    "import os, json, math, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "import torch, torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, log_loss\n",
    "\n",
    "# ---- Paths & device ----\n",
    "PROJECT_ROOT = Path(r\"[path_placeholder]\")\n",
    "RESULTS_DIR  = PROJECT_ROOT / \"results\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---- Preconditions: model, loaders, and weights from prior steps ----\n",
    "assert 'model' in globals(), \"Model not in memory — please run Steps 13–15.\"\n",
    "assert 'test_loader' in globals(), \"test_loader missing — please run Steps 11–12.\"\n",
    "\n",
    "ckpt_ft = RESULTS_DIR / \"mobilenetv2_finetune_tail.pt\"\n",
    "assert ckpt_ft.exists(), f\"Missing {ckpt_ft} — run Step 15.\"\n",
    "\n",
    "# Load best fine-tuned weights safely\n",
    "try:\n",
    "    state = torch.load(ckpt_ft, map_location=\"cpu\", weights_only=True)\n",
    "except TypeError:  # PyTorch < 2.4\n",
    "    state = torch.load(ckpt_ft, map_location=\"cpu\")\n",
    "model.load_state_dict(state)\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# ---- Load calibration artifacts (temperature + operating point) from Step 16 ----\n",
    "op_json = RESULTS_DIR / \"operating_point_val.json\"\n",
    "temp_txt = RESULTS_DIR / \"temperature.txt\"\n",
    "\n",
    "if op_json.exists():\n",
    "    with open(op_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        op_payload = json.load(f)\n",
    "    temperature_T = float(op_payload.get(\"temperature_T\", 1.0))\n",
    "    op_dict = op_payload.get(\"operating_point\", {})\n",
    "    op_mode = op_dict.get(\"mode\", \"sens_at_spec\")\n",
    "    target_specificity = float(op_dict.get(\"target_specificity\", 0.95))\n",
    "    op_threshold = float(op_dict.get(\"threshold\"))\n",
    "else:\n",
    "    # Fallbacks if JSON is missing (shouldn't happen with Step 16 done)\n",
    "    temperature_T = float(Path(temp_txt).read_text().strip()) if temp_txt.exists() else 1.0\n",
    "    op_mode = \"sens_at_spec\"; target_specificity = 0.95\n",
    "    raise RuntimeError(\"operating_point_val.json not found. Please re-run Step 16 to produce it.\")\n",
    "\n",
    "print(f\"Loaded calibration: T={temperature_T:.4f}, mode={op_mode}, \"\n",
    "      f\"target_spec={target_specificity:.2f}, threshold={op_threshold:.4f}\")\n",
    "\n",
    "# ---- Collect logits & labels on the TEST split ----\n",
    "@torch.no_grad()\n",
    "def collect_logits_labels(loader, model, device):\n",
    "    model.eval()\n",
    "    logits_list, y_list = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        out = model(xb).view(-1)  # logits\n",
    "        logits_list.append(out.detach().cpu())\n",
    "        y_list.append(yb.detach().cpu())\n",
    "    logits = torch.cat(logits_list).float()\n",
    "    y      = torch.cat(y_list).long().numpy()\n",
    "    return logits, y\n",
    "\n",
    "test_logits, test_y = collect_logits_labels(test_loader, model, device)\n",
    "\n",
    "# ---- Apply temperature scaling ----\n",
    "class TemperatureScaler(nn.Module):\n",
    "    def __init__(self, T):\n",
    "        super().__init__()\n",
    "        # store as a detached tensor for clean, no-grad forward\n",
    "        self.T = torch.tensor(float(T), dtype=torch.float32)\n",
    "    def forward(self, logits):\n",
    "        return logits / self.T\n",
    "\n",
    "scaler = TemperatureScaler(temperature_T)\n",
    "with torch.no_grad():\n",
    "    test_logits_cal = scaler(test_logits.clone()).view(-1)\n",
    "\n",
    "# ---- Convert to probabilities ----\n",
    "test_p_uncal = torch.sigmoid(test_logits).numpy()\n",
    "test_p_cal   = torch.sigmoid(test_logits_cal).numpy()\n",
    "\n",
    "# ---- Metrics helpers ----\n",
    "def brier_score(probs, y):\n",
    "    probs = np.clip(probs, 1e-6, 1-1e-6)\n",
    "    return float(np.mean((probs - y)**2))\n",
    "\n",
    "def bin_stats_prob_vs_posrate(probs, y, n_bins=10):\n",
    "    probs = np.clip(probs.astype(np.float64), 1e-6, 1-1e-6)\n",
    "    edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    ids   = np.digitize(probs, edges[1:-1])  # 0..n_bins-1\n",
    "    rows  = []\n",
    "    for b in range(n_bins):\n",
    "        m = (ids == b)\n",
    "        n = int(np.sum(m))\n",
    "        lo, hi = float(edges[b]), float(edges[b+1])\n",
    "        if n == 0:\n",
    "            rows.append(dict(bin=b, n=0, bin_lower=lo, bin_upper=hi,\n",
    "                             avg_prob=np.nan, pos_rate=np.nan))\n",
    "        else:\n",
    "            rows.append(dict(\n",
    "                bin=b, n=n, bin_lower=lo, bin_upper=hi,\n",
    "                avg_prob=float(np.mean(probs[m])),\n",
    "                pos_rate=float(np.mean(y[m]))\n",
    "            ))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def ece_binary_probrate(probs, y, n_bins=10):\n",
    "    df = bin_stats_prob_vs_posrate(probs, y, n_bins=n_bins)\n",
    "    n_tot = df[\"n\"].sum()\n",
    "    if n_tot == 0:\n",
    "        return float(\"nan\"), df\n",
    "    gap = np.abs(df[\"pos_rate\"] - df[\"avg_prob\"])\n",
    "    w   = df[\"n\"] / n_tot\n",
    "    ece = float(np.nansum(gap * w))\n",
    "    return ece, df\n",
    "\n",
    "def sens_spec_counts(p, y, thr):\n",
    "    y_hat = (p >= thr).astype(np.int64)\n",
    "    tp = int(np.sum((y_hat==1) & (y==1)))\n",
    "    tn = int(np.sum((y_hat==0) & (y==0)))\n",
    "    fp = int(np.sum((y_hat==1) & (y==0)))\n",
    "    fn = int(np.sum((y_hat==0) & (y==1)))\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else float(\"nan\")\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else float(\"nan\")\n",
    "    return dict(tp=tp, tn=tn, fp=fp, fn=fn, sensitivity=float(sens), specificity=float(spec))\n",
    "\n",
    "# ---- Discrimination (AUC) ----\n",
    "auc_uncal = roc_auc_score(test_y, test_p_uncal)\n",
    "auc_cal   = roc_auc_score(test_y, test_p_cal)\n",
    "\n",
    "# ---- Calibration metrics on TEST (report calibrated; keep uncal for reference) ----\n",
    "nll_uncal = log_loss(test_y, np.clip(test_p_uncal, 1e-6, 1-1e-6))\n",
    "nll_cal   = log_loss(test_y, np.clip(test_p_cal,   1e-6, 1-1e-6))\n",
    "brier_uncal = brier_score(test_p_uncal, test_y)\n",
    "brier_cal   = brier_score(test_p_cal,   test_y)\n",
    "ece_uncal, df_uncal = ece_binary_probrate(test_p_uncal, test_y, n_bins=10)\n",
    "ece_cal,   df_cal   = ece_binary_probrate(test_p_cal,   test_y, n_bins=10)\n",
    "\n",
    "# ---- Operating-point on TEST using *val-derived* threshold ----\n",
    "cm = sens_spec_counts(test_p_cal, test_y, op_threshold)\n",
    "\n",
    "# ---- ROC plot (test) ----\n",
    "fpr_u, tpr_u, _ = roc_curve(test_y, test_p_uncal)\n",
    "fpr_c, tpr_c, _ = roc_curve(test_y, test_p_cal)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot([0,1],[0,1],'--', linewidth=1, label=\"chance\")\n",
    "plt.plot(fpr_u, tpr_u, label=f\"uncalibrated (AUC={auc_uncal:.3f})\")\n",
    "plt.plot(fpr_c, tpr_c, label=f\"calibrated (AUC={auc_cal:.3f})\")\n",
    "plt.xlabel(\"1 - specificity (FPR)\"); plt.ylabel(\"sensitivity (TPR)\")\n",
    "plt.title(\"ROC — test\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "roc_png = RESULTS_DIR / \"roc_test.png\"\n",
    "plt.savefig(roc_png, dpi=150); plt.close()\n",
    "\n",
    "# ---- Reliability diagram (test) ----\n",
    "plt.figure(figsize=(6,4))\n",
    "xs = np.linspace(0,1,101)\n",
    "plt.plot(xs, xs, linestyle=\"--\", linewidth=1, label=\"perfectly calibrated\")\n",
    "for label, df in [(\"uncalibrated\", df_uncal), (\"calibrated\", df_cal)]:\n",
    "    d = df.dropna(subset=[\"avg_prob\",\"pos_rate\"])\n",
    "    plt.plot(d[\"avg_prob\"], d[\"pos_rate\"], marker=\"o\", linewidth=1, label=label)\n",
    "plt.xlabel(\"predicted probability (bin mean)\")\n",
    "plt.ylabel(\"empirical positive rate (bin mean)\")\n",
    "plt.title(\"Reliability diagram — test\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "rel_png = RESULTS_DIR / \"reliability_diagram_test.png\"\n",
    "plt.savefig(rel_png, dpi=150); plt.close()\n",
    "\n",
    "# Save ECE bins for test\n",
    "bins_out = df_uncal.rename(columns={\"avg_prob\":\"uncal_avg_prob\",\"pos_rate\":\"uncal_pos_rate\"})\n",
    "bins_out[[\"cal_avg_prob\",\"cal_pos_rate\"]] = df_cal[[\"avg_prob\",\"pos_rate\"]]\n",
    "bins_csv = RESULTS_DIR / \"ece_bins_test.csv\"\n",
    "bins_out.to_csv(bins_csv, index=False)\n",
    "\n",
    "# ---- Assemble and save JSON report ----\n",
    "report = {\n",
    "    \"dataset\": \"test\",\n",
    "    \"n_examples\": int(len(test_y)),\n",
    "    \"temperature_T\": round(float(temperature_T), 6),\n",
    "    \"operating_point\": {\n",
    "        \"source\": \"validation\",\n",
    "        \"mode\": op_mode,\n",
    "        \"target_specificity\": round(float(target_specificity), 6),\n",
    "        \"threshold\": round(float(op_threshold), 6),\n",
    "        \"test_sensitivity\": round(cm[\"sensitivity\"], 6),\n",
    "        \"test_specificity\": round(cm[\"specificity\"], 6),\n",
    "        \"confusion_matrix\": {k:int(v) for k,v in cm.items() if k in (\"tp\",\"tn\",\"fp\",\"fn\")},\n",
    "    },\n",
    "    \"discrimination\": {\n",
    "        \"auc_uncalibrated\": round(float(auc_uncal), 6),\n",
    "        \"auc_calibrated\":   round(float(auc_cal), 6)\n",
    "    },\n",
    "    \"calibration\": {\n",
    "        \"ece_uncalibrated\": round(float(ece_uncal), 6),\n",
    "        \"ece_calibrated\":   round(float(ece_cal), 6),\n",
    "        \"nll_uncalibrated\": round(float(nll_uncal), 6),\n",
    "        \"nll_calibrated\":   round(float(nll_cal), 6),\n",
    "        \"brier_uncalibrated\": round(float(brier_uncal), 6),\n",
    "        \"brier_calibrated\":   round(float(brier_cal), 6),\n",
    "        \"n_bins\": 10\n",
    "    },\n",
    "    \"artifacts\": {\n",
    "        \"roc_png\": str(roc_png),\n",
    "        \"reliability_diagram_png\": str(rel_png),\n",
    "        \"ece_bins_csv\": str(bins_csv),\n",
    "        \"operating_point_val_json\": str(op_json)\n",
    "    }\n",
    "}\n",
    "\n",
    "out_json = RESULTS_DIR / \"test_metrics.json\"\n",
    "with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "# ---- Console summary ----\n",
    "print(\"\\n=== Test summary (calibrated) ===\")\n",
    "print(f\"AUC (cal): {report['discrimination']['auc_calibrated']:.4f} \"\n",
    "      f\"(uncal: {report['discrimination']['auc_uncalibrated']:.4f})\")\n",
    "print(f\"NLL  (cal): {report['calibration']['nll_calibrated']:.4f}  \"\n",
    "      f\"Brier (cal): {report['calibration']['brier_calibrated']:.4f}  \"\n",
    "      f\"ECE (cal): {report['calibration']['ece_calibrated']:.4f}\")\n",
    "print(f\"Threshold (from val): {report['operating_point']['threshold']:.4f}\")\n",
    "print(f\"Sensitivity@thr: {report['operating_point']['test_sensitivity']:.4f}  \"\n",
    "      f\"Specificity@thr: {report['operating_point']['test_specificity']:.4f}\")\n",
    "print(\"Confusion matrix (TP, TN, FP, FN):\",\n",
    "      report['operating_point']['confusion_matrix'])\n",
    "print(f\"Saved ROC → {roc_png}\")\n",
    "print(f\"Saved reliability diagram → {rel_png}\")\n",
    "print(f\"Saved ECE bins → {bins_csv}\")\n",
    "print(f\"Saved JSON report → {out_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 18 — Uncertainty via TTA + abstention\n",
    "\n",
    "This cell estimates **predictive uncertainty** with **test-time augmentation (TTA)** and defines a simple **abstention rule** to trade coverage for accuracy. It reuses the calibrated model (temperature **T**) and **fixed decision threshold** from validation.\n",
    "\n",
    "- **Prerequisites & artifacts**\n",
    "  - Requires: `splits.csv` (manifests), `mobilenetv2_finetune_tail.pt` (fine-tuned weights), and `operating_point_val.json` (contains `temperature_T`, validation-derived `threshold`, and target specificity).\n",
    "  - Loads the model on CPU/GPU, sets `eval()`, and applies **temperature scaling** at inference only (no retraining).\n",
    "\n",
    "- **Preprocessing (consistent with training)**\n",
    "  - Pads each image to square using the **border-ring modal color** (expected near-black), then **bicubic** resize to **128×128**.\n",
    "  - Converts to tensor and applies **ImageNet normalization** (`mean=[0.485,0.456,0.406]`, `std=[0.229,0.224,0.225]`).\n",
    "\n",
    "- **TTA configuration**\n",
    "  - Uses **mild augmentations** aligned with training: horizontal/vertical flips (p=0.5), ±10° rotation (fill=0), brightness/contrast jitter (~[0.8, 1.2]).\n",
    "  - For each image, samples **N=8** augmented variants, obtains logits, divides by **T** (from Step 16), applies `sigmoid`, and computes:\n",
    "    - `mean_p`: mean probability across TTA samples.\n",
    "    - `std_p`: standard deviation across TTA samples.\n",
    "  - Runs TTA separately for **validation** and **test** subsets.\n",
    "\n",
    "- **Baseline (no abstention)**\n",
    "  - Computes the **baseline accuracy** by thresholding `mean_p` with the **fixed operating threshold** from validation (`OP_THR`).\n",
    "\n",
    "- **Abstention rule (two-dimensional)**\n",
    "  - Decision to **keep** a prediction:\n",
    "    - Keep if `|mean_p − 0.5| ≥ δ` **AND** `std_p ≤ σ_thr`.\n",
    "    - Otherwise **abstain** (defer/flag as uncertain).\n",
    "  - Intuition:\n",
    "    - `|mean_p − 0.5|` measures **confidence margin** away from ambiguity.\n",
    "    - `std_p` captures **instability** across plausible views (data uncertainty).\n",
    "\n",
    "- **Validation-driven selection of (δ, σ_thr)**\n",
    "  - **Grid search** on validation for `δ ∈ [0.00, 0.30]` and `σ_thr ∈ [0.00, 0.10]`.\n",
    "  - Objective: meet **target coverage** (default **0.90** kept) and **maximize accuracy among kept**.\n",
    "  - If no pair hits coverage, selects the closest-by-coverage candidate, then highest accuracy (deterministic tie-break).\n",
    "\n",
    "- **Frontier plot & artifacts**\n",
    "  - Builds a **coverage–accuracy frontier** on validation and highlights the selected point.\n",
    "  - Saves:\n",
    "    - `coverage_accuracy_curve.png` — frontier with selected (δ, σ_thr).\n",
    "    - `abstention_rule.json` — reusable rule:\n",
    "      - `\"rule\": \"abstain if |mean_p - 0.5| < delta OR std_p > sigma_thr\"`\n",
    "      - Selected `delta`, `sigma_thr`, `tta` details (N and aug), `calibration_T`, fixed `classification_threshold`, validation `target_specificity`, and target coverage.\n",
    "\n",
    "- **Summaries on val & test**\n",
    "  - Applies the chosen (δ, σ_thr) to **both validation and test**:\n",
    "    - Reports **coverage** (fraction kept), **n_kept / n_abstained**, **accuracy among kept**, **baseline accuracy** (no abstention), and **accuracy gain**.\n",
    "  - Saves:\n",
    "    - `abstention_val_summary.json`\n",
    "    - `abstention_test_summary.json`\n",
    "  - Prints a concise console summary (rule, parameters, coverage, accuracy-kept, baseline).\n",
    "\n",
    "- **Why this step**\n",
    "  - Adds an **operational safety lever**: the system can **abstain** on uncertain cases, improving reliability at controlled throughput (coverage), without modifying the classifier or threshold learned on validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 18 — Uncertainty via TTA + abstention\n",
    "import json, math, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "# ---- Roots & prerequisites (consistent with earlier steps) ----\n",
    "PROJECT_ROOT = Path(r\"[path_placeholder]\")\n",
    "RESULTS_DIR  = PROJECT_ROOT / \"results\"\n",
    "SPLITS_CSV   = RESULTS_DIR / \"splits.csv\"\n",
    "OP_JSON      = RESULTS_DIR / \"operating_point_val.json\"\n",
    "CKPT_FT      = RESULTS_DIR / \"mobilenetv2_finetune_tail.pt\"\n",
    "assert SPLITS_CSV.exists() and OP_JSON.exists() and CKPT_FT.exists(), \"Run Steps 15–17 first.\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---- Load calibration artifacts (T and operating threshold from Step 16) ----\n",
    "with open(OP_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    op_payload = json.load(f)\n",
    "T_cal      = float(op_payload[\"temperature_T\"])\n",
    "op_dict    = op_payload[\"operating_point\"]\n",
    "OP_THR     = float(op_dict[\"threshold\"])            # same threshold as the rest of the pipeline\n",
    "TARGET_SPEC = float(op_dict.get(\"target_specificity\", 0.95))  # for metadata only\n",
    "\n",
    "# ---- (Re)load best fine-tuned weights & eval mode ----\n",
    "# Assumes 'model' from Step 15 exists; if not, rebuild MobileNetV2 as in Step 13 and load ft weights.\n",
    "try:\n",
    "    model\n",
    "except NameError:\n",
    "    from torchvision import models\n",
    "    import torch.nn as nn\n",
    "    weights = models.MobileNet_V2_Weights.DEFAULT\n",
    "    model = models.mobilenet_v2(weights=weights)\n",
    "    in_features = model.classifier[1].in_features\n",
    "    model.classifier[1] = nn.Linear(in_features, 1)\n",
    "\n",
    "state = torch.load(CKPT_FT, map_location=\"cpu\")\n",
    "model.load_state_dict(state)\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# ---- Mild TTA: same ranges as training ----\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "to_tensor = T.ToTensor()\n",
    "normalise = T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "tta_aug   = T.Compose([\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomVerticalFlip(p=0.5),\n",
    "    T.RandomRotation(degrees=10, fill=0),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2),  # ~[0.8,1.2]\n",
    "])\n",
    "\n",
    "def make_tensor_from_path(path: Path, augment: bool):\n",
    "    with Image.open(path).convert(\"RGB\") as img:\n",
    "        img = pad_resize_canonical(img, target_size=128, ring=2)\n",
    "    if augment:\n",
    "        img = tta_aug(img)\n",
    "    x = to_tensor(img)\n",
    "    x = normalise(x)\n",
    "    return x\n",
    "\n",
    "# ---- Compute TTA (mean prob & std) for a split ----\n",
    "def tta_split(df_split: pd.DataFrame, N=8, batch_device=device):\n",
    "    \"\"\"Returns dict with keys: y (0/1), mean_p, std_p\"\"\"\n",
    "    ys, mean_ps, std_ps = [], [], []\n",
    "    bce_sigmoid = torch.nn.Sigmoid()\n",
    "    scaler_T = 1.0 / T_cal\n",
    "\n",
    "    for _, row in df_split.iterrows():\n",
    "        path = Path(row[\"path\"])\n",
    "        y    = 1 if row[\"class\"] == \"Parasitized\" else 0\n",
    "\n",
    "        # Build one TTA batch of size N for this image\n",
    "        xb = torch.stack([ make_tensor_from_path(path, augment=True) for _ in range(N) ], dim=0)\n",
    "        xb = xb.to(batch_device, non_blocking=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(xb).view(-1) * scaler_T   # temperature scaling on logits\n",
    "            p = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "\n",
    "        ys.append(y)\n",
    "        mean_ps.append(float(np.mean(p)))\n",
    "        std_ps.append(float(np.std(p)))\n",
    "\n",
    "    return {\n",
    "        \"y\": np.array(ys, dtype=np.int64),\n",
    "        \"mean_p\": np.array(mean_ps, dtype=np.float32),\n",
    "        \"std_p\": np.array(std_ps, dtype=np.float32),\n",
    "    }\n",
    "\n",
    "# ---- Load splits ----\n",
    "splits = pd.read_csv(SPLITS_CSV)\n",
    "val_df  = splits[splits[\"split\"]==\"val\"].reset_index(drop=True)\n",
    "test_df = splits[splits[\"split\"]==\"test\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"TTA running… N=8  |  val={len(val_df)}  test={len(test_df)}\")\n",
    "t0 = time.time()\n",
    "val_stats  = tta_split(val_df,  N=8)\n",
    "test_stats = tta_split(test_df, N=8)\n",
    "print(f\"Done in {time.time()-t0:.1f}s\")\n",
    "\n",
    "# ---- Helper: evaluate abstention rule ----\n",
    "def eval_rule(ys, mean_p, std_p, thr, delta, sigma_thr):\n",
    "    keep = (np.abs(mean_p - 0.5) >= delta) & (std_p <= sigma_thr)\n",
    "    cov  = float(np.mean(keep)) if len(keep) else 0.0\n",
    "    if cov > 0:\n",
    "        preds = (mean_p[keep] >= thr).astype(np.int64)\n",
    "        acc   = float(np.mean(preds == ys[keep]))\n",
    "    else:\n",
    "        acc = float(\"nan\")\n",
    "    return cov, acc\n",
    "\n",
    "# Baseline (no abstention) for reference\n",
    "def baseline_acc(ys, mean_p, thr):\n",
    "    return float(np.mean((mean_p >= thr).astype(np.int64) == ys))\n",
    "\n",
    "val_baseline_acc  = baseline_acc(val_stats[\"y\"],  val_stats[\"mean_p\"],  OP_THR)\n",
    "test_baseline_acc = baseline_acc(test_stats[\"y\"], test_stats[\"mean_p\"], OP_THR)\n",
    "\n",
    "# ---- Grid search on validation for (delta, sigma_thr) at target coverage ----\n",
    "TARGET_COVERAGE = 0.90   # \"modest coverage reduction\"; change to 0.85 if you want stronger abstention\n",
    "\n",
    "delta_grid = np.linspace(0.00, 0.30, 31)   # |p-0.5| band\n",
    "sigma_grid = np.linspace(0.00, 0.10, 21)   # std over N=8\n",
    "\n",
    "grid_rows = []\n",
    "best = None  # (acc, coverage, delta, sigma_thr)\n",
    "for d in delta_grid:\n",
    "    for s in sigma_grid:\n",
    "        cov, acc = eval_rule(val_stats[\"y\"], val_stats[\"mean_p\"], val_stats[\"std_p\"], OP_THR, d, s)\n",
    "        grid_rows.append({\"delta\": float(d), \"sigma_thr\": float(s), \"coverage\": cov, \"accuracy\": acc})\n",
    "        if cov >= TARGET_COVERAGE:\n",
    "            if best is None or acc > best[0] or (math.isclose(acc, best[0]) and cov > best[1]):\n",
    "                best = (acc, cov, d, s)\n",
    "\n",
    "# Fallback if nothing meets coverage target: take the point with coverage closest *above* target if any,\n",
    "# otherwise the closest overall, then highest accuracy.\n",
    "if best is None:\n",
    "    # distance to target coverage, then -accuracy (so we prefer higher accuracy)\n",
    "    ranked = sorted(grid_rows, key=lambda r: (abs(r[\"coverage\"]-TARGET_COVERAGE), -r[\"accuracy\"]))\n",
    "    top = ranked[0]\n",
    "    best = (top[\"accuracy\"], top[\"coverage\"], top[\"delta\"], top[\"sigma_thr\"])\n",
    "\n",
    "best_acc, best_cov, BEST_DELTA, BEST_SIGMA = best\n",
    "\n",
    "# ---- Build coverage–accuracy curve (Pareto frontier on val) & plot ----\n",
    "grid_df = pd.DataFrame(grid_rows).dropna()\n",
    "# Pareto frontier: sort by coverage desc, keep points with strictly increasing accuracy\n",
    "frontier = []\n",
    "grid_sorted = grid_df.sort_values([\"coverage\",\"accuracy\"], ascending=[True, False]).values.tolist()\n",
    "# sweep from low→high coverage, keep running max accuracy\n",
    "covs = []; accs = []\n",
    "for cov in np.linspace(grid_df[\"coverage\"].min(), 1.0, 100):\n",
    "    subset = grid_df[grid_df[\"coverage\"]>=cov]\n",
    "    if len(subset):\n",
    "        covs.append(cov)\n",
    "        accs.append(subset[\"accuracy\"].max())\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(covs, accs, linewidth=2, label=\"validation frontier\")\n",
    "plt.scatter(grid_df[\"coverage\"], grid_df[\"accuracy\"], s=8, alpha=0.25, label=\"grid\")\n",
    "plt.scatter([best_cov], [best_acc], s=60, marker=\"o\", label=f\"selected (δ={BEST_DELTA:.3f}, σ={BEST_SIGMA:.3f})\")\n",
    "plt.axvline(TARGET_COVERAGE, linestyle=\"--\", linewidth=1, label=f\"target coverage={TARGET_COVERAGE:.2f}\")\n",
    "plt.xlabel(\"coverage (kept fraction)\"); plt.ylabel(\"accuracy among kept\")\n",
    "plt.title(\"Coverage vs accuracy — validation\")\n",
    "plt.legend()\n",
    "cov_curve_png = RESULTS_DIR / \"coverage_accuracy_curve.png\"\n",
    "plt.tight_layout(); plt.savefig(cov_curve_png, dpi=150); plt.close()\n",
    "\n",
    "# ---- Save rule for reuse ----\n",
    "rule_json = {\n",
    "    \"rule\": \"abstain if |mean_p - 0.5| < delta OR std_p > sigma_thr\",\n",
    "    \"delta\": round(float(BEST_DELTA), 6),\n",
    "    \"sigma_thr\": round(float(BEST_SIGMA), 6),\n",
    "    \"tta\": {\"N\": 8, \"augs\": \"H/V flip, ±10° rot, brightness/contrast [0.8,1.2]\"},\n",
    "    \"calibration_T\": round(T_cal, 6),\n",
    "    \"classification_threshold\": round(OP_THR, 6),\n",
    "    \"target_specificity_from_val\": round(TARGET_SPEC, 6),\n",
    "    \"target_coverage\": TARGET_COVERAGE\n",
    "}\n",
    "with open(RESULTS_DIR / \"abstention_rule.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(rule_json, f, indent=2)\n",
    "\n",
    "# ---- Summaries (val & test) using the selected rule ----\n",
    "def summarize_split(name, stats, thr, delta, sigma_thr, baseline_acc):\n",
    "    cov, acc = eval_rule(stats[\"y\"], stats[\"mean_p\"], stats[\"std_p\"], thr, delta, sigma_thr)\n",
    "    kept = ((np.abs(stats[\"mean_p\"] - 0.5) >= delta) & (stats[\"std_p\"] <= sigma_thr))\n",
    "    n_all = int(stats[\"y\"].shape[0]); n_kept = int(kept.sum()); n_abst = n_all - n_kept\n",
    "    preds = (stats[\"mean_p\"][kept] >= thr).astype(np.int64) if n_kept else np.array([], dtype=np.int64)\n",
    "    acc_gain = acc - baseline_acc if n_kept else 0.0\n",
    "    return {\n",
    "        \"split\": name,\n",
    "        \"n_total\": n_all,\n",
    "        \"coverage\": round(cov, 6),\n",
    "        \"n_kept\": n_kept,\n",
    "        \"n_abstained\": n_abst,\n",
    "        \"accuracy_kept\": round(acc, 6),\n",
    "        \"baseline_accuracy_no_abstention\": round(baseline_acc, 6),\n",
    "        \"accuracy_gain_vs_baseline\": round(acc_gain, 6),\n",
    "        \"delta\": round(float(delta), 6),\n",
    "        \"sigma_thr\": round(float(sigma_thr), 6),\n",
    "        \"threshold_used\": round(float(thr), 6)\n",
    "    }\n",
    "\n",
    "val_summary  = summarize_split(\"val\",  val_stats,  OP_THR, BEST_DELTA, BEST_SIGMA, val_baseline_acc)\n",
    "test_summary = summarize_split(\"test\", test_stats, OP_THR, BEST_DELTA, BEST_SIGMA, test_baseline_acc)\n",
    "\n",
    "with open(RESULTS_DIR / \"abstention_val_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(val_summary, f, indent=2)\n",
    "with open(RESULTS_DIR / \"abstention_test_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(test_summary, f, indent=2)\n",
    "\n",
    "print(\"\\n=== Abstention (selected rule) ===\")\n",
    "print(\"Rule: abstain if |mean_p - 0.5| < δ OR std_p > σ_thr\")\n",
    "print(f\"δ={BEST_DELTA:.3f}, σ_thr={BEST_SIGMA:.3f}, target coverage={TARGET_COVERAGE:.2f}\")\n",
    "print(f\"Validation: coverage={val_summary['coverage']:.3f}, acc_kept={val_summary['accuracy_kept']:.4f} \"\n",
    "      f\"(baseline={val_summary['baseline_accuracy_no_abstention']:.4f})\")\n",
    "print(f\"Test:       coverage={test_summary['coverage']:.3f}, acc_kept={test_summary['accuracy_kept']:.4f} \"\n",
    "      f\"(baseline={test_summary['baseline_accuracy_no_abstention']:.4f})\")\n",
    "print(f\"Saved rule → {RESULTS_DIR/'abstention_rule.json'}\")\n",
    "print(f\"Saved curve → {cov_curve_png}\")\n",
    "print(f\"Saved summaries → {RESULTS_DIR/'abstention_val_summary.json'}, {RESULTS_DIR/'abstention_test_summary.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 19 — Out-of-distribution (OOD) flag via Maximum Softmax Probability (MSP)\n",
    "\n",
    "This cell adds an OOD detector on top of the calibrated classifier using **MSP** (for binary models: `max(p, 1−p)` from calibrated logits). It selects an MSP threshold on the **validation** set to achieve a target **in-distribution TPR** (95%), then reports OOD performance on **test** using that fixed threshold.\n",
    "\n",
    "- **Purpose**\n",
    "  - Identify inputs that are likely **out-of-distribution** relative to training/validation data.\n",
    "  - Provide an **ID-vs-OOD score** (MSP ∈ [0.5, 1.0]) and a **decision threshold** `τ` tuned on validation.\n",
    "\n",
    "- **Prerequisites & setup**\n",
    "  - Uses: `splits.csv`, `mobilenetv2_finetune_tail.pt` (fine-tuned weights), `operating_point_val.json` (for temperature `T`).\n",
    "  - Reproducibility: `seed_everything(19)` (Python/NumPy/Torch/CUDA) and fixed seeds for OOD generators.\n",
    "  - Device: CPU or CUDA; model is put in `eval()`.\n",
    "\n",
    "- **Model & calibration**\n",
    "  - Rebuilds MobileNetV2 with a **single-logit head** and loads fine-tuned weights.\n",
    "  - Applies **temperature scaling** at inference (`logits / T_cal`) to ensure probabilities are aligned with previous calibration.\n",
    "\n",
    "- **Preprocessing (consistent with training)**\n",
    "  - **Pad-to-square** using the **border-ring modal color** (typically near-black), then **bicubic** resize to **128×128**.\n",
    "  - Convert to tensor and apply **ImageNet normalization**.\n",
    "\n",
    "- **Synthetic OOD construction (for evaluation only)**\n",
    "  - Creates **corrupted variants** of in-distribution images to approximate OOD:\n",
    "    - Heavy Gaussian blur (radius ~6–9).\n",
    "    - Brightness extremes (0.2×, 1.8×).\n",
    "    - Contrast extremes (0.2×, 2.5×).\n",
    "    - With 30% probability, **compose two** corruptions.\n",
    "  - Processes the same validation/test file list twice: once as **ID** (clean) and once as **synthetic OOD** (corrupted).\n",
    "\n",
    "- **Scoring: MSP (binary)**\n",
    "  - From calibrated logits: `p = sigmoid(logits / T)`.\n",
    "  - **MSP = max(p, 1−p)**; higher ⇒ more **ID-like**.\n",
    "  - Range: **[0.5, 1.0]** (0.5 = maximally uncertain).\n",
    "\n",
    "- **Threshold selection (validation)**\n",
    "  - Target **TPR(ID) = 0.95** → set `τ` to the **5th percentile** of validation **ID** MSP (`quantile = 1 − TPR_TARGET`).\n",
    "  - At `τ`, report **TNR(OOD)** on the synthetic validation OOD set.\n",
    "  - Save: `ood_threshold.json` (score type, `threshold`, achieved `val_tpr`, `val_tnr`, and `temperature_T`).\n",
    "  - Plot: `ood_hist_msp_id_vs_ood.png` (MSP histograms for ID vs synthetic OOD with the vertical `τ` line).\n",
    "\n",
    "- **Test-time evaluation (fixed τ from validation)**\n",
    "  - Compute MSP for **test ID** and **test synthetic OOD**.\n",
    "  - Metrics:\n",
    "    - **AUROC (ID=1 vs OOD=0)** using MSP (ID should have **higher** scores).\n",
    "    - **TNR@95%TPR** on test using the **validation-selected τ**.\n",
    "  - Save: `ood_metrics_test.json` (counts, AUROC, TPR/TNR, threshold, temperature, notes).\n",
    "\n",
    "- **Artifacts**\n",
    "  - `results/ood_threshold.json` — Selected `τ` at 95% ID TPR on validation.\n",
    "  - `results/ood_hist_msp_id_vs_ood.png` — Validation MSP histograms with `τ`.\n",
    "  - `results/ood_metrics_test.json` — Test AUROC and TNR@95%TPR (with `τ` fixed from validation).\n",
    "\n",
    "- **Operational notes**\n",
    "  - **Assumption**: synthetic corruptions approximate OOD; real-world OOD can differ.\n",
    "  - **Decision semantics**: MSP ≥ `τ` ⇒ **flag as ID**; MSP < `τ` ⇒ **flag as OOD**.\n",
    "  - **Separation of concerns**: OOD decision is **independent** from the disease classification threshold; it only gates whether the classifier should be trusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 19 — OOD flag via MSP\n",
    "import os, json, math, time, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms as T\n",
    "from torchvision.transforms import functional as TF\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ---------- Paths & constants (keep consistent with earlier steps) ----------\n",
    "PROJECT_ROOT = Path(r\"[path_placeholder]\")\n",
    "DATA_DIR     = PROJECT_ROOT / \"cell_images\"\n",
    "RESULTS_DIR  = PROJECT_ROOT / \"results\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SPLITS_CSV   = RESULTS_DIR / \"splits.csv\"\n",
    "CKPT_FT      = RESULTS_DIR / \"mobilenetv2_finetune_tail.pt\"\n",
    "OP_JSON      = RESULTS_DIR / \"operating_point_val.json\"\n",
    "\n",
    "assert SPLITS_CSV.exists(), f\"Missing {SPLITS_CSV}\"\n",
    "assert CKPT_FT.exists(),    f\"Missing {CKPT_FT}\"\n",
    "assert OP_JSON.exists(),    f\"Missing {OP_JSON} (run Step 16)\"\n",
    "\n",
    "# ---------- Reproducibility ----------\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "seed_everything(19)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---------- Load T and model ----------\n",
    "with open(OP_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    op_payload = json.load(f)\n",
    "T_cal = float(op_payload.get(\"temperature_T\", 1.0))\n",
    "\n",
    "# Build MobileNetV2 (single-logit head) and load fine-tuned weights\n",
    "def build_mobilenetv2_single_logit(pretrained=False):\n",
    "    try:\n",
    "        weights = models.MobileNet_V2_Weights.DEFAULT if pretrained else None\n",
    "        net = models.mobilenet_v2(weights=weights)\n",
    "    except Exception:\n",
    "        net = models.mobilenet_v2(pretrained=pretrained)\n",
    "    in_features = net.classifier[1].in_features\n",
    "    net.classifier[1] = nn.Linear(in_features, 1)\n",
    "    return net\n",
    "\n",
    "model = build_mobilenetv2_single_logit(pretrained=False)\n",
    "try:\n",
    "    state = torch.load(CKPT_FT, map_location=\"cpu\", weights_only=True)  # PyTorch ≥2.4\n",
    "except TypeError:\n",
    "    state = torch.load(CKPT_FT, map_location=\"cpu\")\n",
    "model.load_state_dict(state)\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# ---------- Canonical preprocessing (same as training/inference) ----------\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "to_tensor = T.ToTensor()\n",
    "normalize = T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "\n",
    "def preprocess_to_tensor(path: Path):\n",
    "    with Image.open(path).convert(\"RGB\") as img:\n",
    "        img = pad_resize_canonical(img, target_size=128, ring=2)\n",
    "    x = to_tensor(img)\n",
    "    x = normalize(x)\n",
    "    return x\n",
    "\n",
    "# ---------- Synthetic OOD generator ----------\n",
    "def make_synthetic_ood(img: Image.Image, rng: np.random.RandomState):\n",
    "    \"\"\"Apply one (or two) strong corruptions to a 128×128 PIL image.\"\"\"\n",
    "    ops = []\n",
    "    # heavy blur\n",
    "    def op_blur(i):\n",
    "        r = float(rng.uniform(6.0, 9.0))\n",
    "        return i.filter(ImageFilter.GaussianBlur(radius=r))\n",
    "    # brightness extremes\n",
    "    def op_bright_low(i):  return TF.adjust_brightness(i, 0.2)\n",
    "    def op_bright_high(i): return TF.adjust_brightness(i, 1.8)\n",
    "    # contrast extremes\n",
    "    def op_contrast_low(i):  return TF.adjust_contrast(i, 0.2)\n",
    "    def op_contrast_high(i): return TF.adjust_contrast(i, 2.5)\n",
    "\n",
    "    choices = [op_blur, op_bright_low, op_bright_high, op_contrast_low, op_contrast_high]\n",
    "    i1 = rng.choice(len(choices))\n",
    "    img = choices[i1](img)\n",
    "    # With 30% chance, compose a second corruption\n",
    "    if rng.rand() < 0.30:\n",
    "        i2 = rng.choice(len(choices))\n",
    "        img = choices[i2](img)\n",
    "    return img\n",
    "\n",
    "def preprocess_to_tensor_ood(path: Path, rng: np.random.RandomState):\n",
    "    with Image.open(path).convert(\"RGB\") as img:\n",
    "        img = pad_resize_canonical(img, target_size=128, ring=2)\n",
    "        img = make_synthetic_ood(img, rng)\n",
    "    x = to_tensor(img)\n",
    "    x = normalize(x)\n",
    "    return x\n",
    "\n",
    "# ---------- Data (val/test) ----------\n",
    "splits = pd.read_csv(SPLITS_CSV)\n",
    "val_df  = splits[splits[\"split\"]==\"val\"].reset_index(drop=True)\n",
    "test_df = splits[splits[\"split\"]==\"test\"].reset_index(drop=True)\n",
    "print(f\"val={len(val_df)}  test={len(test_df)}\")\n",
    "\n",
    "# ---------- Batched inference helpers ----------\n",
    "@torch.no_grad()\n",
    "def logits_for_paths(paths, batch=128):\n",
    "    xs = []\n",
    "    out = []\n",
    "    for i, p in enumerate(paths, 1):\n",
    "        xs.append(preprocess_to_tensor(Path(p)))\n",
    "        if len(xs) == batch or i == len(paths):\n",
    "            xb = torch.stack(xs, 0).to(device, non_blocking=True)\n",
    "            logits = model(xb).view(-1)\n",
    "            out.append(logits.detach().cpu())\n",
    "            xs = []\n",
    "    return torch.cat(out).float()\n",
    "\n",
    "@torch.no_grad()\n",
    "def logits_for_paths_ood(paths, seed=123, batch=128):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    xs = []\n",
    "    out = []\n",
    "    for i, p in enumerate(paths, 1):\n",
    "        xs.append(preprocess_to_tensor_ood(Path(p), rng))\n",
    "        if len(xs) == batch or i == len(paths):\n",
    "            xb = torch.stack(xs, 0).to(device, non_blocking=True)\n",
    "            logits = model(xb).view(-1)\n",
    "            out.append(logits.detach().cpu())\n",
    "            xs = []\n",
    "    return torch.cat(out).float()\n",
    "\n",
    "def msp_from_logits(logits: torch.Tensor, T: float):\n",
    "    \"\"\"Return MSP for binary model given logits and temperature T.\"\"\"\n",
    "    logits_T = logits / float(T)\n",
    "    p = torch.sigmoid(logits_T).numpy()\n",
    "    msp = np.maximum(p, 1.0 - p)\n",
    "    return msp\n",
    "\n",
    "# ---------- Compute MSP on val (ID) and val-OOD, choose τ for TPR=0.95 ----------\n",
    "print(\"Collecting logits… (validation)\")\n",
    "val_logits_id  = logits_for_paths(val_df[\"path\"].tolist(), batch=256)\n",
    "val_logits_ood = logits_for_paths_ood(val_df[\"path\"].tolist(), seed=777, batch=256)\n",
    "\n",
    "msp_val_id  = msp_from_logits(val_logits_id,  T_cal)\n",
    "msp_val_ood = msp_from_logits(val_logits_ood, T_cal)\n",
    "\n",
    "TPR_TARGET = 0.95\n",
    "# Threshold τ so that 95% of ID have MSP ≥ τ  → τ is the 5th percentile of ID MSP\n",
    "tau = float(np.quantile(msp_val_id, 1.0 - TPR_TARGET))\n",
    "\n",
    "# Validation TNR at this τ\n",
    "val_pred_id  = (msp_val_id  >= tau)\n",
    "val_pred_ood = (msp_val_ood >= tau)  # predicted \"ID\" if True\n",
    "val_tpr = float(np.mean(val_pred_id))                 # by construction ≈ 0.95\n",
    "val_tnr = float(np.mean(~val_pred_ood))               # true negatives among OOD\n",
    "\n",
    "# Save τ\n",
    "thr_json = {\n",
    "    \"score\": \"MSP\",\n",
    "    \"threshold\": round(tau, 6),\n",
    "    \"tpr_target_id\": TPR_TARGET,\n",
    "    \"val_tpr_id\": round(val_tpr, 6),\n",
    "    \"val_tnr_ood_at_tpr\": round(val_tnr, 6),\n",
    "    \"temperature_T\": round(float(T_cal), 6)\n",
    "}\n",
    "with open(RESULTS_DIR / \"ood_threshold.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(thr_json, f, indent=2)\n",
    "\n",
    "# ---------- Histogram (val) ----------\n",
    "plt.figure(figsize=(7,4.2))\n",
    "bins = np.linspace(0.5, 1.0, 40)  # MSP ∈ [0.5, 1]\n",
    "plt.hist(msp_val_id,  bins=bins, alpha=0.65, label=\"ID (val)\")\n",
    "plt.hist(msp_val_ood, bins=bins, alpha=0.65, label=\"synthetic OOD (val)\")\n",
    "plt.axvline(tau, linestyle=\"--\", linewidth=1.5, label=f\"τ @ 95% TPR (ID) = {tau:.3f}\")\n",
    "plt.xlabel(\"MSP\"); plt.ylabel(\"count\")\n",
    "plt.title(\"MSP histogram — ID vs synthetic OOD (validation)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "hist_png = RESULTS_DIR / \"ood_hist_msp_id_vs_ood.png\"\n",
    "plt.savefig(hist_png, dpi=150); plt.close()\n",
    "print(f\"Saved histogram → {hist_png}\")\n",
    "\n",
    "# ---------- Test metrics using val-selected τ ----------\n",
    "print(\"Collecting logits… (test and test-OOD)\")\n",
    "test_logits_id  = logits_for_paths(test_df[\"path\"].tolist(), batch=256)\n",
    "test_logits_ood = logits_for_paths_ood(test_df[\"path\"].tolist(), seed=888, batch=256)\n",
    "\n",
    "msp_test_id  = msp_from_logits(test_logits_id,  T_cal)\n",
    "msp_test_ood = msp_from_logits(test_logits_ood, T_cal)\n",
    "\n",
    "# AUROC (ID=1, OOD=0) with MSP (higher means more ID-like)\n",
    "y_test = np.concatenate([np.ones_like(msp_test_id), np.zeros_like(msp_test_ood)])\n",
    "s_test = np.concatenate([msp_test_id, msp_test_ood])\n",
    "auroc = float(roc_auc_score(y_test, s_test))\n",
    "\n",
    "# TNR@95%TPR on TEST using τ from validation\n",
    "tpr_test = float(np.mean(msp_test_id >= tau))\n",
    "tnr_test = float(np.mean(msp_test_ood <  tau))\n",
    "\n",
    "metrics = {\n",
    "    \"temperature_T\": round(float(T_cal), 6),\n",
    "    \"threshold\": round(float(tau), 6),\n",
    "    \"n_test_id\": int(len(msp_test_id)),\n",
    "    \"n_test_ood\": int(len(msp_test_ood)),\n",
    "    \"auroc_id_vs_ood\": round(auroc, 6),\n",
    "    \"tpr_id_at_tau\": round(tpr_test, 6),\n",
    "    \"tnr_ood_at_95pct_tpr\": round(tnr_test, 6),\n",
    "    \"notes\": \"ID positive class; score = MSP; τ chosen on validation as 5th percentile of ID MSP.\"\n",
    "}\n",
    "with open(RESULTS_DIR / \"ood_metrics_test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "# ---------- Console summary ----------\n",
    "print(\"\\n=== OOD via MSP ===\")\n",
    "print(f\"T (temperature): {T_cal:.4f}\")\n",
    "print(f\"τ (MSP) @ 95% TPR (selected on val): {tau:.4f}\")\n",
    "print(f\"Validation:  TPR(ID)={val_tpr:.3f}  TNR(OOD)={val_tnr:.3f}\")\n",
    "print(f\"Test AUROC (ID vs OOD): {auroc:.4f}\")\n",
    "print(f\"Test:        TPR(ID)={tpr_test:.3f}  TNR(OOD)={tnr_test:.3f}\")\n",
    "print(f\"Saved → {RESULTS_DIR/'ood_threshold.json'}, {hist_png}, {RESULTS_DIR/'ood_metrics_test.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 20 — Photometric robustness sweeps (test set)\n",
    "\n",
    "This cell measures how sensitive the **calibrated** classifier is to controlled **brightness** and **contrast** shifts on the **test set**, keeping the decision threshold fixed from validation. It sweeps multiplicative factors around nominal (1.0), reports **AUC** and **Accuracy@threshold**, and saves summary artifacts.\n",
    "\n",
    "- **Inputs & prerequisites**\n",
    "  - Uses paths and artifacts from prior steps: `splits.csv` (test manifest), `mobilenetv2_finetune_tail.pt` (fine-tuned weights), and `operating_point_val.json` (calibration temperature `T` and classification threshold `OP_THR`).\n",
    "  - Runs on CPU or CUDA (`eval()` mode).\n",
    "\n",
    "- **Calibration & decision rule**\n",
    "  - Loads **temperature** `T_cal` and the **operating threshold** `OP_THR` selected on validation.\n",
    "  - At inference, applies **temperature scaling** to logits (`logits / T_cal`) before `sigmoid`.\n",
    "  - **Accuracy** is computed by thresholding calibrated probabilities with **the same `OP_THR` across all perturbations** to ensure a fair comparison.\n",
    "\n",
    "- **Model & preprocessing (aligned with training)**\n",
    "  - Rebuilds MobileNetV2 with a **single-logit** classifier; loads fine-tuned weights.\n",
    "  - Preprocesses each image by **pad-to-square** using the image’s border-ring modal color (typically near-black), **bicubic** resize to **128×128**, then **ImageNet normalization**.\n",
    "\n",
    "- **Photometric perturbations**\n",
    "  - Two one-parameter families applied **after** pad/resize and **before** tensor conversion:\n",
    "    - **Brightness** factors: `[0.8, 0.9, 1.0, 1.1, 1.2]` via `TF.adjust_brightness`.\n",
    "    - **Contrast** factors: `[0.8, 0.9, 1.0, 1.1, 1.2]` via `TF.adjust_contrast`.\n",
    "  - `1.0` is nominal; `<1.0` darkens/lowers contrast, `>1.0` brightens/raises contrast.\n",
    "\n",
    "- **Evaluation protocol**\n",
    "  - For each factor in each family:\n",
    "    - Run **batched** inference on all test images with temperature scaling.\n",
    "    - Compute:\n",
    "      - **AUC** (threshold-free discrimination).\n",
    "      - **Accuracy@OP_THR** (performance at the fixed operating point).\n",
    "  - Aggregates all results into a single table and prints per-factor summaries.\n",
    "\n",
    "- **Saved artifacts**\n",
    "  - `results/robustness_metrics.csv` — Per-factor metrics with the used `OP_THR` and `T_cal`.\n",
    "  - `results/robustness_brightness_curve.png` — AUC and Accuracy vs. **brightness** factor (vertical line at 1.0).\n",
    "  - `results/robustness_contrast_curve.png` — AUC and Accuracy vs. **contrast** factor (vertical line at 1.0).\n",
    "\n",
    "- **Interpretation notes**\n",
    "  - **AUC** reflects ranking robustness; **Accuracy@threshold** shows stability at the chosen clinical/operational point.\n",
    "  - Drops at factors far from 1.0 indicate **sensitivity** to illumination or contrast shifts; symmetric/asymmetric patterns may reveal bias to over/under-exposure.\n",
    "  - Because the **threshold is fixed from validation**, changes isolate **input shift effects** rather than retuning.\n",
    "  - Consider pairing with **data augmentation**, histogram equalization, or calibration refresh if large degradations are observed.\n",
    "\n",
    "- **Extensibility**\n",
    "  - Additional sweeps (e.g., gamma, saturation, hue, JPEG quality) can be added by following the same pattern and appending rows to the metrics table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 20 — Photometric robustness sweeps (test set)\n",
    "import json, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "from torchvision import models, transforms as T\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "# ---- Roots & prerequisites (reuse your paths) ----\n",
    "PROJECT_ROOT = Path(r\"[path_placeholder]\")\n",
    "DATA_DIR     = PROJECT_ROOT / \"cell_images\"\n",
    "RESULTS_DIR  = PROJECT_ROOT / \"results\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SPLITS_CSV   = RESULTS_DIR / \"splits.csv\"\n",
    "CKPT_FT      = RESULTS_DIR / \"mobilenetv2_finetune_tail.pt\"\n",
    "OP_JSON      = RESULTS_DIR / \"operating_point_val.json\"\n",
    "\n",
    "assert SPLITS_CSV.exists(), \"Missing splits.csv — run earlier steps.\"\n",
    "assert CKPT_FT.exists(),    \"Missing fine-tuned weights — run Step 15.\"\n",
    "assert OP_JSON.exists(),    \"Missing operating_point_val.json — run Step 16.\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---- Load calibration: temperature T and decision threshold (from Step 16) ----\n",
    "with open(OP_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    op_payload = json.load(f)\n",
    "T_cal   = float(op_payload[\"temperature_T\"])\n",
    "op_dict = op_payload[\"operating_point\"]\n",
    "OP_THR  = float(op_dict[\"threshold\"])\n",
    "print(f\"Loaded calibration → T={T_cal:.4f}, threshold={OP_THR:.4f}\")\n",
    "\n",
    "# ---- Build model & load best weights ----\n",
    "def build_mobilenetv2_single_logit(pretrained=False):\n",
    "    try:\n",
    "        weights = models.MobileNet_V2_Weights.DEFAULT if pretrained else None\n",
    "        net = models.mobilenet_v2(weights=weights)\n",
    "    except Exception:\n",
    "        net = models.mobilenet_v2(pretrained=pretrained)\n",
    "    in_features = net.classifier[1].in_features\n",
    "    net.classifier[1] = nn.Linear(in_features, 1)\n",
    "    return net\n",
    "\n",
    "model = build_mobilenetv2_single_logit(pretrained=False)\n",
    "try:\n",
    "    state = torch.load(CKPT_FT, map_location=\"cpu\", weights_only=True)\n",
    "except TypeError:\n",
    "    state = torch.load(CKPT_FT, map_location=\"cpu\")\n",
    "model.load_state_dict(state)\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# ---- Canonical preprocessing (pad-to-square using border mode + bicubic 128²) ----\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "to_tensor = T.ToTensor()\n",
    "normalize = T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "\n",
    "def preprocess_with_adjust(path: Path, mode: str, factor: float):\n",
    "    with Image.open(path).convert(\"RGB\") as img:\n",
    "        img = pad_resize_canonical(img, target_size=128, ring=2)\n",
    "        if mode == \"brightness\":\n",
    "            img = TF.adjust_brightness(img, float(factor))\n",
    "        elif mode == \"contrast\":\n",
    "            img = TF.adjust_contrast(img, float(factor))\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'brightness' or 'contrast'\")\n",
    "    x = to_tensor(img)\n",
    "    x = normalize(x)\n",
    "    return x\n",
    "\n",
    "# ---- Batched inference helper (applies T for calibrated probs) ----\n",
    "@torch.no_grad()\n",
    "def probs_for_paths(paths, mode: str, factor: float, batch=256):\n",
    "    xs, out = [], []\n",
    "    invT = 1.0 / float(T_cal)  # divide logits by T\n",
    "    for i, p in enumerate(paths, 1):\n",
    "        xs.append(preprocess_with_adjust(Path(p), mode=mode, factor=factor))\n",
    "        if len(xs) == batch or i == len(paths):\n",
    "            xb = torch.stack(xs, 0).to(device, non_blocking=True)\n",
    "            logits = model(xb).view(-1) * invT\n",
    "            out.append(torch.sigmoid(logits).cpu().numpy())\n",
    "            xs = []\n",
    "    return np.concatenate(out)\n",
    "\n",
    "# ---- Load test paths & labels ----\n",
    "splits = pd.read_csv(SPLITS_CSV)\n",
    "test_df = splits[splits[\"split\"]==\"test\"].reset_index(drop=True)\n",
    "label_map = {\"Parasitized\": 1, \"Uninfected\": 0}\n",
    "y_test = test_df[\"class\"].map(label_map).to_numpy().astype(np.int64)\n",
    "paths   = test_df[\"path\"].tolist()\n",
    "print(f\"Test set size: {len(paths)}\")\n",
    "\n",
    "# ---- Metric helpers ----\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "def metrics_at_factor(mode: str, factor: float):\n",
    "    p = probs_for_paths(paths, mode=mode, factor=factor, batch=256)\n",
    "    auc = float(roc_auc_score(y_test, p))\n",
    "    acc = float(accuracy_score(y_test, (p >= OP_THR).astype(np.int64)))\n",
    "    return auc, acc\n",
    "\n",
    "# ---- Sweeps ----\n",
    "BR_FACTORS = [0.8, 0.9, 1.0, 1.1, 1.2]\n",
    "CT_FACTORS = [0.8, 0.9, 1.0, 1.1, 1.2]\n",
    "\n",
    "rows = []\n",
    "\n",
    "print(\"\\nBrightness sweep:\")\n",
    "for f in BR_FACTORS:\n",
    "    auc, acc = metrics_at_factor(\"brightness\", f)\n",
    "    rows.append({\"type\":\"brightness\", \"factor\":f, \"auc\":auc, \"acc_at_threshold\":acc, \"n\":int(len(y_test)),\n",
    "                 \"threshold_used\": float(OP_THR), \"temperature_T\": float(T_cal)})\n",
    "    print(f\"  factor={f:.2f}  AUC={auc:.4f}  Acc@thr={acc:.4f}\")\n",
    "\n",
    "print(\"\\nContrast sweep:\")\n",
    "for f in CT_FACTORS:\n",
    "    auc, acc = metrics_at_factor(\"contrast\", f)\n",
    "    rows.append({\"type\":\"contrast\", \"factor\":f, \"auc\":auc, \"acc_at_threshold\":acc, \"n\":int(len(y_test)),\n",
    "                 \"threshold_used\": float(OP_THR), \"temperature_T\": float(T_cal)})\n",
    "    print(f\"  factor={f:.2f}  AUC={auc:.4f}  Acc@thr={acc:.4f}\")\n",
    "\n",
    "robust_df = pd.DataFrame(rows)\n",
    "csv_out = RESULTS_DIR / \"robustness_metrics.csv\"\n",
    "robust_df.to_csv(csv_out, index=False)\n",
    "print(f\"\\nSaved metrics → {csv_out}\")\n",
    "\n",
    "# ---- Curves (single-axes plots with both lines, vertical guide at 1.0) ----\n",
    "def plot_curve(df, kind: str, out_path: Path, title: str):\n",
    "    sub = df[df[\"type\"] == kind].sort_values(\"factor\")\n",
    "    x = sub[\"factor\"].to_numpy()\n",
    "    y_auc = sub[\"auc\"].to_numpy()\n",
    "    y_acc = sub[\"acc_at_threshold\"].to_numpy()\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(x, y_auc, marker=\"o\", label=\"AUC\")\n",
    "    plt.plot(x, y_acc, marker=\"s\", label=f\"Accuracy@thr={OP_THR:.3f}\")\n",
    "    plt.axvline(1.0, linestyle=\"--\", linewidth=1, label=\"nominal (1.0)\")\n",
    "    plt.xlabel(f\"{kind} factor\"); plt.ylabel(\"score\")\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150); plt.close()\n",
    "\n",
    "plot_curve(robust_df, \"brightness\",\n",
    "           RESULTS_DIR / \"robustness_brightness_curve.png\",\n",
    "           \"Photometric robustness — brightness (test)\")\n",
    "plot_curve(robust_df, \"contrast\",\n",
    "           RESULTS_DIR / \"robustness_contrast_curve.png\",\n",
    "           \"Photometric robustness — contrast (test)\")\n",
    "\n",
    "print(\"Saved →\",\n",
    "      RESULTS_DIR / \"robustness_brightness_curve.png\",\n",
    "      \"and\",\n",
    "      RESULTS_DIR / \"robustness_contrast_curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 21 — Interpretability (Grad-CAM)\n",
    "\n",
    "This cell generates **Grad-CAM** explanations for the fine-tuned MobileNetV2 on the **test split**, targeting the last convolutional block (`features[18]`). It saves (i) a **4×4 panel** with up to four examples each for **TP / FP / TN / FN**, and (ii) **per-case overlays**.\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose\n",
    "- Provide **visual evidence** of what the model attends to when predicting *Parasitized* vs *Uninfected*.\n",
    "- Inspect **correct** (TP/TN) vs **error** (FP/FN) cases under the **same calibrated operating point** used in evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs & prerequisites\n",
    "- Artifacts from earlier steps:\n",
    "  - `results/splits.csv` (test manifest with file paths and labels).\n",
    "  - `results/mobilenetv2_finetune_tail.pt` (fine-tuned weights).\n",
    "  - `results/operating_point_val.json` (temperature `T_CAL` and decision threshold `OP_THR`).\n",
    "- Rebuilds MobileNetV2 with a **single-logit** head and loads the fine-tuned weights.\n",
    "- Runs on CPU or CUDA (`eval()` mode).\n",
    "\n",
    "---\n",
    "\n",
    "## Preprocessing (consistent with training/inference)\n",
    "- **Pad-to-square** using the **border-ring modal color** (typically near-black), then **bicubic** resize to **128×128**.\n",
    "- Convert to tensor and apply **ImageNet normalization**.\n",
    "- Two products per image:\n",
    "  - **PIL 128×128** for visualization overlays.\n",
    "  - **Normalized tensor** for the forward/backward pass.\n",
    "\n",
    "---\n",
    "\n",
    "## Grad-CAM details\n",
    "- **Target layer:** `model.features[-1]` (last conv block before pooling/classifier).\n",
    "- **Hooks:** forward hook stores activations; backward hook stores gradients.\n",
    "- **Signal for backprop:** the **temperature-scaled logit** (`logit / T_CAL`) used to compute the probability at the operating point. This keeps the explanation consistent with calibrated inference.\n",
    "- **Weights:** global average of gradients over spatial dims → channel weights.\n",
    "- **CAM construction:** ReLU of the weighted sum of feature maps; upsample to **128×128**; min-max normalize to **[0,1]**.\n",
    "- **Overlay:** Blend the heatmap (e.g., `\"jet\"`) with the processed PIL image (α = 0.35).\n",
    "\n",
    "---\n",
    "\n",
    "## Case selection & panel assembly\n",
    "- Computes **calibrated probability** `p = sigmoid(logit / T_CAL)` and **predicted class** via `p ≥ OP_THR`.\n",
    "- Partitions test indices into **TP / FP / TN / FN** and samples up to **4** per category (deterministic seeds).\n",
    "- For each sampled case:\n",
    "  - Run Grad-CAM, create **overlay**, annotate with **true label**, **predicted label**, and **p**.\n",
    "  - Save an individual PNG under `results/gradcam_cases/`.\n",
    "- Assemble a **4×4 grid** (rows = TP/FP/TN/FN; up to 4 columns each). Empty categories are left blank.\n",
    "\n",
    "---\n",
    "\n",
    "## Outputs\n",
    "- `results/gradcam_panel.png` — 4×4 panel of overlays.\n",
    "- `results/gradcam_cases/*.png` — per-image overlays named with category, stem, and probability.\n",
    "\n",
    "---\n",
    "\n",
    "## How to read the overlays\n",
    "- **Warmer regions** indicate areas contributing **positively** to the predicted class for that image (after calibration).\n",
    "- **TP/TN:** Expect focus on **cell interiors/morphology** rather than borders or padding.\n",
    "- **FP/FN:** Highlights on irrelevant background, staining artifacts, or borders may indicate **spurious cues**.\n",
    "- Resolution (128²) and the chosen layer provide **coarse**, class-specific attributions; Grad-CAM is **qualitative**, not a proof of causality.\n",
    "\n",
    "---\n",
    "\n",
    "## Notes & limitations\n",
    "- If a category has **< 4** instances, remaining slots are blank; if a category is **empty**, its row is blank.\n",
    "- CAMs depend on the **chosen layer**; earlier layers can yield finer spatial detail at the cost of specificity.\n",
    "- Color maps and α are fixed for consistency; modifying them changes visual salience but not the underlying CAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 21 — Interpretability (Grad-CAM)\n",
    "# Scope: last conv block of MobileNetV2 (features[18]).\n",
    "# Outputs:\n",
    "#   results/gradcam_panel.png    (4x4 grid: TP/FP/TN/FN, 4 each)\n",
    "#   results/gradcam_cases/*.png  (per-case overlays)\n",
    "\n",
    "import os, json, math, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms as T\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "# ---- Roots & artifacts from prior steps ----\n",
    "PROJECT_ROOT = Path(r\"[path_placeholder]\")\n",
    "DATA_DIR     = PROJECT_ROOT / \"cell_images\"\n",
    "RESULTS_DIR  = PROJECT_ROOT / \"results\"\n",
    "SPLITS_CSV   = RESULTS_DIR / \"splits.csv\"\n",
    "CKPT_FT      = RESULTS_DIR / \"mobilenetv2_finetune_tail.pt\"\n",
    "OP_JSON      = RESULTS_DIR / \"operating_point_val.json\"\n",
    "\n",
    "assert SPLITS_CSV.exists() and CKPT_FT.exists() and OP_JSON.exists(), \"Run Steps 15–17 first.\"\n",
    "(RESULTS_DIR / \"gradcam_cases\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---- Load temperature + operating threshold ----\n",
    "with open(OP_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    payload = json.load(f)\n",
    "T_CAL   = float(payload[\"temperature_T\"])\n",
    "OP_THR  = float(payload[\"operating_point\"][\"threshold\"])\n",
    "\n",
    "# ---- Build model, load weights ----\n",
    "def build_mobilenetv2_single_logit(pretrained=False):\n",
    "    try:\n",
    "        weights = models.MobileNet_V2_Weights.DEFAULT if pretrained else None\n",
    "        net = models.mobilenet_v2(weights=weights)\n",
    "    except Exception:\n",
    "        net = models.mobilenet_v2(pretrained=pretrained)\n",
    "    in_features = net.classifier[1].in_features\n",
    "    net.classifier[1] = nn.Linear(in_features, 1)\n",
    "    return net\n",
    "\n",
    "model = build_mobilenetv2_single_logit(pretrained=False)\n",
    "try:\n",
    "    state = torch.load(CKPT_FT, map_location=\"cpu\", weights_only=True)  # PyTorch ≥2.4\n",
    "except TypeError:\n",
    "    state = torch.load(CKPT_FT, map_location=\"cpu\")\n",
    "model.load_state_dict(state)\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# ---- Preprocess (same as training/inference) ----\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "to_tensor = T.ToTensor()\n",
    "normalize = T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "\n",
    "def preprocess_both(path: Path):\n",
    "    \"\"\"Return (PIL_128x128 for overlay, tensor_normalised for model).\"\"\"\n",
    "    with Image.open(path).convert(\"RGB\") as img:\n",
    "        img = pad_resize_canonical(img, target_size=128, ring=2)  # 128×128 PIL\n",
    "    x = to_tensor(img)            # [0,1]\n",
    "    x = normalize(x)              # ImageNet normalisation\n",
    "    return img, x\n",
    "\n",
    "# ---- Grad-CAM helper focused on features[18] ----\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.activ = None\n",
    "        self.grads = None\n",
    "        self.fh = target_layer.register_forward_hook(self._fwd_hook)\n",
    "        # full backward hook for modern PyTorch; fallback to deprecated if unavailable\n",
    "        try:\n",
    "            self.bh = target_layer.register_full_backward_hook(self._bwd_hook)\n",
    "        except Exception:\n",
    "            self.bh = target_layer.register_backward_hook(self._bwd_hook)\n",
    "\n",
    "    def _fwd_hook(self, module, inp, out):\n",
    "        self.activ = out.detach()\n",
    "\n",
    "    def _bwd_hook(self, module, grad_in, grad_out):\n",
    "        self.grads = grad_out[0].detach()\n",
    "\n",
    "    def __call__(self, x_tensor, use_temperature=True):\n",
    "        \"\"\"\n",
    "        x_tensor: [1,3,128,128] normalised\n",
    "        returns: (cam[128,128] np.float32 in [0,1], prob, logit_cal, pred_label[0/1])\n",
    "        \"\"\"\n",
    "        self.model.zero_grad(set_to_none=True)\n",
    "        logits = self.model(x_tensor).view(-1)  # [1]\n",
    "        # Apply temperature on logits for the *prediction/probability* path\n",
    "        if use_temperature:\n",
    "            logits_for_pred = logits / float(T_CAL)\n",
    "        else:\n",
    "            logits_for_pred = logits\n",
    "        prob = torch.sigmoid(logits_for_pred)[0].item()\n",
    "\n",
    "        # For Grad-CAM, backprop from the calibrated logit (keeps consistency with OP)\n",
    "        logits_for_pred.backward(retain_graph=True)\n",
    "\n",
    "        A = self.activ           # [1,C,h,w]\n",
    "        dA = self.grads          # [1,C,h,w]\n",
    "        weights = dA.mean(dim=(2,3), keepdim=True)   # [1,C,1,1]\n",
    "        cam = torch.relu((weights * A).sum(dim=1, keepdim=True))  # [1,1,h,w]\n",
    "        cam = F.interpolate(cam, size=(128,128), mode=\"bilinear\", align_corners=False)\n",
    "        cam = cam[0,0]\n",
    "        # normalise to [0,1]\n",
    "        cam -= cam.min()\n",
    "        denom = (cam.max() + 1e-8)\n",
    "        cam = (cam / denom).clamp(0,1).cpu().numpy().astype(np.float32)\n",
    "        pred = int(prob >= OP_THR)\n",
    "        return cam, prob, logits_for_pred.item(), pred\n",
    "\n",
    "    def close(self):\n",
    "        self.fh.remove()\n",
    "        self.bh.remove()\n",
    "\n",
    "# Target: last conv block before GAP\n",
    "target_layer = model.features[-1]   # features[18]\n",
    "gcam = GradCAM(model, target_layer)\n",
    "\n",
    "# ---- Collect test set predictions to pick TP/FP/TN/FN ----\n",
    "splits = pd.read_csv(SPLITS_CSV)\n",
    "test_df = splits[splits[\"split\"]==\"test\"].reset_index(drop=True)\n",
    "label_map = {\"Parasitized\": 1, \"Uninfected\": 0}\n",
    "\n",
    "records = []\n",
    "with torch.no_grad():\n",
    "    for i, row in test_df.iterrows():\n",
    "        path = Path(row[\"path\"])\n",
    "        y = label_map[row[\"class\"]]\n",
    "        _, x = preprocess_both(path)\n",
    "        x = x.unsqueeze(0).to(device)\n",
    "        logit = model(x).view(-1)\n",
    "        p = torch.sigmoid(logit / float(T_CAL)).item()\n",
    "        pred = int(p >= OP_THR)\n",
    "        records.append({\"idx\": i, \"path\": str(path), \"y\": y, \"p\": p, \"pred\": pred})\n",
    "\n",
    "pred_df = pd.DataFrame(records)\n",
    "\n",
    "# Index by category\n",
    "TP_idx = pred_df[(pred_df[\"pred\"]==1) & (pred_df[\"y\"]==1)].index.tolist()\n",
    "FP_idx = pred_df[(pred_df[\"pred\"]==1) & (pred_df[\"y\"]==0)].index.tolist()\n",
    "TN_idx = pred_df[(pred_df[\"pred\"]==0) & (pred_df[\"y\"]==0)].index.tolist()\n",
    "FN_idx = pred_df[(pred_df[\"pred\"]==0) & (pred_df[\"y\"]==1)].index.tolist()\n",
    "\n",
    "def sample_idxs(idxs, k=4, seed=123):\n",
    "    if len(idxs) <= k:\n",
    "        return idxs\n",
    "    rng = np.random.RandomState(seed)\n",
    "    return list(rng.choice(idxs, size=k, replace=False))\n",
    "\n",
    "TP_s = sample_idxs(TP_idx, 4, 1)\n",
    "FP_s = sample_idxs(FP_idx, 4, 2)\n",
    "TN_s = sample_idxs(TN_idx, 4, 3)\n",
    "FN_s = sample_idxs(FN_idx, 4, 4)\n",
    "\n",
    "panel_order = [(\"TP\", TP_s), (\"FP\", FP_s), (\"TN\", TN_s), (\"FN\", FN_s)]\n",
    "\n",
    "# ---- Utilities to overlay CAM on the processed 128×128 image ----\n",
    "def overlay_cam_on_image(pil_img_128, cam_128, alpha=0.35, cmap_name=\"jet\"):\n",
    "    \"\"\"Return a PIL RGB image with heatmap overlay.\"\"\"\n",
    "    import matplotlib.cm as cm\n",
    "    base = np.asarray(pil_img_128).astype(np.float32) / 255.0  # [H,W,3]\n",
    "    cmap = cm.get_cmap(cmap_name)\n",
    "    heat = cmap(cam_128)[:, :, :3]   # RGBA->RGB\n",
    "    overlay = (1 - alpha) * base + alpha * heat\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "    return Image.fromarray((overlay * 255).astype(np.uint8))\n",
    "\n",
    "# ---- Generate per-case overlays & assemble panel ----\n",
    "fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "plt.subplots_adjust(wspace=0.02, hspace=0.15)\n",
    "\n",
    "case_dir = RESULTS_DIR / \"gradcam_cases\"\n",
    "case_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for r, (label, idxs) in enumerate(panel_order):\n",
    "    if len(idxs) == 0:\n",
    "        # If a category is missing, fill with blanks\n",
    "        for c in range(4):\n",
    "            axes[r, c].axis(\"off\")\n",
    "        continue\n",
    "    for c, df_idx in enumerate(idxs[:4]):\n",
    "        row = pred_df.loc[df_idx]\n",
    "        path = Path(row[\"path\"])\n",
    "        true_y = int(row[\"y\"])\n",
    "        pil_img, x = preprocess_both(path)\n",
    "        x = x.unsqueeze(0).to(device).requires_grad_(True)\n",
    "\n",
    "        cam, prob, logit_cal, pred = gcam(x, use_temperature=True)\n",
    "        over = overlay_cam_on_image(pil_img, cam, alpha=0.35, cmap_name=\"jet\")\n",
    "\n",
    "        # Save per-case\n",
    "        base_name = f\"{label}_{path.stem}_p{prob:.3f}.png\"\n",
    "        over.save(case_dir / base_name)\n",
    "\n",
    "        # Title with class/pred/prob\n",
    "        gt_txt   = \"Parasitized\" if true_y==1 else \"Uninfected\"\n",
    "        pred_txt = \"Parasitized\" if pred==1 else \"Uninfected\"\n",
    "        axes[r, c].imshow(over)\n",
    "        axes[r, c].set_title(f\"{label} | gt={gt_txt} | pred={pred_txt}\\np={prob:.3f}\", fontsize=9)\n",
    "        axes[r, c].axis(\"off\")\n",
    "\n",
    "# Row headers if we sampled <4 in a row: fill remaining cells\n",
    "for r in range(4):\n",
    "    for c in range(4):\n",
    "        if not hasattr(axes[r, c], 'has_data') or not axes[r, c].images:\n",
    "            axes[r, c].axis(\"off\")\n",
    "\n",
    "# Add left y-labels for rows\n",
    "for r, (label, _) in enumerate(panel_order):\n",
    "    axes[r, 0].text(-0.10, 0.5, label, transform=axes[r, 0].transAxes,\n",
    "                    fontsize=12, fontweight=\"bold\", va=\"center\", ha=\"right\", rotation=90)\n",
    "\n",
    "out_png = RESULTS_DIR / \"gradcam_panel.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_png, dpi=150, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "gcam.close()\n",
    "\n",
    "print(f\"Saved panel → {out_png}\")\n",
    "print(f\"Saved per-case overlays → {case_dir}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP4iUINar29r37zhq9IAW/W",
   "gpuType": "T4",
   "mount_file_id": "1Q9oHP6m3-wwgCpRMkdreKiTa9RjXXQpZ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11 (.venv) DDLS",
   "language": "python",
   "name": "ddls-local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
